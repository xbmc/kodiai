---
phase: 97-multi-llm-routing-cost-tracking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llm/task-types.ts
  - src/llm/providers.ts
  - src/llm/task-router.ts
  - src/llm/pricing.ts
  - src/llm/index.ts
  - src/execution/config.ts
autonomous: true
requirements:
  - LLM-02
  - LLM-03

must_haves:
  truths:
    - "Task types use dot-separated hierarchy (review.full, slack.response, cluster.label)"
    - "TaskRouter resolves a task type string to a concrete model+provider+sdk tuple"
    - "Wildcard matching works (review.* matches review.full, review.summary)"
    - "Exact match takes priority over wildcard match"
    - ".kodiai.yml models: section parsed and validated by config schema"
    - "Provider factory creates AI SDK model instances from model ID strings"
    - "Pricing config loaded from JSON file, not hardcoded in source"
  artifacts:
    - path: "src/llm/task-types.ts"
      provides: "TaskType constants and dot-separated hierarchy definitions"
      exports: ["TASK_TYPES", "TaskType", "isAgenticTaskType"]
    - path: "src/llm/task-router.ts"
      provides: "Task router resolving task type to model+provider"
      exports: ["createTaskRouter", "ResolvedModel"]
    - path: "src/llm/providers.ts"
      provides: "Provider model factory for AI SDK instances"
      exports: ["createProviderModel", "extractProvider", "validateProviderKeys"]
    - path: "src/llm/pricing.ts"
      provides: "Pricing config loader and cost estimation function"
      exports: ["loadPricing", "estimateCost", "ModelPricing"]
    - path: "src/llm/index.ts"
      provides: "Barrel exports for the llm module"
    - path: "src/execution/config.ts"
      provides: "Extended repoConfigSchema with models: section"
      contains: "modelsSchema"
  key_links:
    - from: "src/llm/task-router.ts"
      to: "src/execution/config.ts"
      via: "models config from RepoConfig.models"
      pattern: "models.*Record.*string"
    - from: "src/llm/providers.ts"
      to: "@ai-sdk/anthropic, @ai-sdk/openai, @ai-sdk/google"
      via: "provider package imports"
      pattern: "import.*@ai-sdk"
---

<objective>
Install AI SDK packages and build the task routing foundation layer: task type taxonomy, provider registry, task router with wildcard resolution, pricing configuration, and .kodiai.yml models: schema extension.

Purpose: Establish the routing infrastructure that Plan 03 will wire into the execution path. All downstream LLM calls need a resolved model before they can route or track costs.
Output: `src/llm/` module with task types, router, providers, pricing; extended config schema.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/97-multi-llm-routing-cost-tracking/97-CONTEXT.md
@.planning/phases/97-multi-llm-routing-cost-tracking/97-RESEARCH.md
@src/execution/config.ts
@src/execution/executor.ts

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/execution/config.ts (lines 460-478):
```typescript
const repoConfigSchema = z.object({
  model: z.string().default("claude-sonnet-4-5-20250929"),
  maxTurns: z.number().min(1).max(100).default(25),
  timeoutSeconds: z.number().min(30).max(1800).default(600),
  systemPromptAppend: z.string().optional(),
  write: writeSchema,
  review: reviewSchema,
  mention: mentionSchema,
  telemetry: telemetrySchema,
  knowledge: knowledgeSchema,
  languageRules: languageRulesSchema,
  largePR: largePRSchema,
  feedback: feedbackSchema,
  timeout: timeoutSchema,
});
export type RepoConfig = z.infer<typeof repoConfigSchema>;
```

From src/execution/executor.ts (line 36):
```typescript
const model = context.modelOverride ?? config.model;
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK packages and create task types + provider registry</name>
  <files>
    package.json
    src/llm/task-types.ts
    src/llm/providers.ts
  </files>
  <action>
**Step 1: Install AI SDK packages.**
```bash
bun add ai@^6.0 @ai-sdk/anthropic@^3.0 @ai-sdk/openai@^3.0 @ai-sdk/google@^3.0
```

**Step 2: Create `src/llm/task-types.ts`.**

Define the dot-separated task type taxonomy per CONTEXT.md locked decisions:
- `review.full` -- Full PR review (agentic, Agent SDK default)
- `review.summary` -- PR summary label generation (non-agentic)
- `mention.response` -- @mention handling (agentic, Agent SDK default)
- `slack.response` -- Slack thread responses (agentic, Agent SDK default)
- `cluster.label` -- Cluster label generation (non-agentic, future Phase 100)
- `staleness.evidence` -- Wiki staleness evaluation (non-agentic, future Phase 99)

Export:
- `TASK_TYPES` -- const object with all task type string literals
- `TaskType` -- union type of all valid task type strings
- `isAgenticTaskType(taskType: string): boolean` -- returns true for task types that default to Agent SDK (review.full, mention.response, slack.response). These are task types that use MCP tools and ephemeral workspaces.
- `AGENTIC_TASK_TYPES: Set<string>` -- the set of agentic task type strings

**Step 3: Create `src/llm/providers.ts`.**

Build a provider model factory per research Pattern 3:
- `createProviderModel(modelId: string)` -- maps model ID string to AI SDK provider model instance:
  - `claude-*` or `anthropic/*` prefix -> `anthropic(modelId)`
  - `gpt-*`, `o3-*`, `o4-*`, or `openai/*` prefix -> `openai(modelId)`
  - `gemini-*` or `google/*` prefix -> `google(modelId)`
  - Default: `anthropic(modelId)` (existing default provider)
  - Strip provider prefix (e.g., `anthropic/claude-sonnet-4-20250514` -> `claude-sonnet-4-20250514`)
- `extractProvider(modelId: string): string` -- returns "anthropic", "openai", or "google" from a model ID string
- `validateProviderKeys(modelIds: string[], logger: Logger): void` -- at startup, check env vars (ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_GENERATIVE_AI_API_KEY) for all referenced providers. Log a warning (not crash) for missing keys per research Pitfall 3.

Import provider constructors:
```typescript
import { anthropic } from "@ai-sdk/anthropic";
import { openai } from "@ai-sdk/openai";
import { google } from "@ai-sdk/google";
```

IMPORTANT: Do NOT use `streamText()` anywhere. Bun production build failure (oven-sh/bun#25630). Use `generateText()` exclusively in downstream plans.
  </action>
  <verify>
    <automated>cd /home/keith/src/kodiai && bun run tsc --noEmit 2>&1 | head -30</automated>
  </verify>
  <done>AI SDK packages installed. task-types.ts exports TASK_TYPES, TaskType, isAgenticTaskType. providers.ts exports createProviderModel, extractProvider, validateProviderKeys. Both compile without type errors.</done>
</task>

<task type="auto">
  <name>Task 2: Create task router, pricing config, config schema extension, and barrel index</name>
  <files>
    src/llm/task-router.ts
    src/llm/pricing.ts
    src/llm/index.ts
    src/execution/config.ts
  </files>
  <action>
**Step 1: Create `src/llm/pricing.ts`.**

Build a configuration-driven pricing module (NOT hardcoded in source per CONTEXT.md decision):
- Create `src/llm/pricing.json` with initial pricing data:
  ```json
  {
    "lastUpdated": "2026-02-25",
    "models": {
      "claude-sonnet-4-20250514": { "inputPerMillion": 3.00, "outputPerMillion": 15.00 },
      "claude-sonnet-4-5-20250929": { "inputPerMillion": 3.00, "outputPerMillion": 15.00 },
      "claude-haiku-4-5-20250929": { "inputPerMillion": 0.80, "outputPerMillion": 4.00 },
      "gpt-4o-mini": { "inputPerMillion": 0.15, "outputPerMillion": 0.60 },
      "o3-mini": { "inputPerMillion": 1.10, "outputPerMillion": 4.40 },
      "gemini-2.0-flash": { "inputPerMillion": 0.10, "outputPerMillion": 0.40 }
    }
  }
  ```
- Export types: `ModelPricing`, `PricingConfig`
- `loadPricing(): PricingConfig` -- loads pricing.json, logs warning if `lastUpdated` > 30 days old
- `estimateCost(model: string, inputTokens: number, outputTokens: number): number` -- computes USD cost using pricing config. Returns 0 if model not found in pricing (fail-open).
- `getModelPricing(model: string): ModelPricing | null` -- returns pricing for a specific model

**Step 2: Create `src/llm/task-router.ts`.**

Build the TaskRouter per research Pattern 1 and CONTEXT.md locked decisions:
- `ResolvedModel` type:
  ```typescript
  interface ResolvedModel {
    modelId: string;
    provider: string;
    sdk: "agent" | "ai";
    fallbackModelId: string;
    fallbackProvider: string;
  }
  ```
- `createTaskRouter(config: { models: Record<string, string>; defaultModel?: string; defaultFallbackModel?: string })` factory function returning:
  - `resolve(taskType: string): ResolvedModel` -- resolution order per research Pitfall 5:
    1. Exact match: `config.models["review.full"]`
    2. Longest prefix wildcard: `config.models["review.*"]`
    3. Category default: agentic tasks -> `claude-sonnet-4-5-20250929` via Agent SDK, non-agentic -> `claude-haiku-4-5-20250929` via AI SDK
    4. Global default: `config.defaultModel ?? "claude-sonnet-4-5-20250929"`
  - When resolving `sdk` field: if the resolved model is a non-Claude model AND the task type is agentic, set `sdk: "ai"` but log a warning: "Agentic task {taskType} routed away from Agent SDK -- MCP tools unavailable"
  - If the task type is agentic AND the resolved model is Claude, set `sdk: "agent"`
  - For non-agentic tasks, always set `sdk: "ai"`
  - Fallback model defaults to `claude-sonnet-4-5-20250929` (existing default) unless overridden in config
  - Extract provider using `extractProvider()` from providers.ts

**Step 3: Extend `src/execution/config.ts` with `models:` schema section.**

Add to `repoConfigSchema`:
```typescript
const modelsSchema = z
  .record(z.string(), z.string())
  .default({});
```

Add `models: modelsSchema` to the `repoConfigSchema` z.object() call, after the existing fields. This supports `.kodiai.yml` like:
```yaml
models:
  review.full: claude-sonnet-4-20250514
  slack.response: gpt-4o-mini
  "review.*": claude-haiku-4-5-20250929
```

Also add optional `defaultModel` and `defaultFallbackModel` fields:
```typescript
defaultModel: z.string().optional(),
defaultFallbackModel: z.string().optional(),
```

**Step 4: Create `src/llm/index.ts` barrel.**

Export all public APIs from the llm module:
- From task-types: TASK_TYPES, TaskType, isAgenticTaskType, AGENTIC_TASK_TYPES
- From task-router: createTaskRouter, ResolvedModel
- From providers: createProviderModel, extractProvider, validateProviderKeys
- From pricing: loadPricing, estimateCost, getModelPricing, ModelPricing
  </action>
  <verify>
    <automated>cd /home/keith/src/kodiai && bun run tsc --noEmit 2>&1 | head -30</automated>
  </verify>
  <done>Task router resolves task types with exact > wildcard > category default > global default precedence. Config schema accepts `models:`, `defaultModel`, `defaultFallbackModel`. Pricing loaded from JSON config file. Barrel index exports all public APIs. TypeScript compiles clean.</done>
</task>

</tasks>

<verification>
- `bun run tsc --noEmit` passes with zero errors
- `src/llm/` directory contains: task-types.ts, providers.ts, task-router.ts, pricing.ts, pricing.json, index.ts
- `src/execution/config.ts` repoConfigSchema includes `models`, `defaultModel`, `defaultFallbackModel` fields
- AI SDK packages present in package.json: ai, @ai-sdk/anthropic, @ai-sdk/openai, @ai-sdk/google
</verification>

<success_criteria>
- TaskRouter.resolve("review.full") returns a ResolvedModel with sdk:"agent" and appropriate model
- TaskRouter.resolve("cluster.label") returns a ResolvedModel with sdk:"ai"
- Config with `models: { "review.full": "gpt-4o-mini" }` causes resolve("review.full") to return gpt-4o-mini with sdk:"ai" and a warning
- estimateCost("claude-sonnet-4-5-20250929", 1000, 500) returns a non-zero number
- loadRepoConfig() with a `models:` section produces a valid config
</success_criteria>

<output>
After completion, create `.planning/phases/97-multi-llm-routing-cost-tracking/97-01-SUMMARY.md`
</output>
