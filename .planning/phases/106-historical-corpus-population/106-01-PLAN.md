---
phase: 106-historical-corpus-population
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/migrations/015-issue-sync-state.sql
  - src/db/migrations/015-issue-sync-state.down.sql
  - src/knowledge/issue-backfill.ts
  - src/knowledge/issue-backfill.test.ts
  - src/knowledge/issue-comment-chunker.ts
  - src/knowledge/issue-comment-chunker.test.ts
autonomous: true
requirements:
  - INGEST-01
  - INGEST-02
  - INGEST-03
  - INGEST-04
  - INGEST-05

must_haves:
  truths:
    - "Backfill engine paginates GitHub Issues API, filters PRs, embeds issues, and upserts via IssueStore"
    - "Backfill engine persists sync state after each page for cursor-based resume"
    - "Backfill engine logs structured progress (page count, issues processed, embeddings created, rate limit remaining)"
    - "Long comments are chunked with overlap, not truncated"
    - "Bot comments are filtered out before embedding"
  artifacts:
    - path: "src/db/migrations/015-issue-sync-state.sql"
      provides: "issue_sync_state table for resume tracking"
      contains: "CREATE TABLE.*issue_sync_state"
    - path: "src/knowledge/issue-backfill.ts"
      provides: "Core backfill engine with pagination, embedding, sync state, rate limiting"
      exports: ["backfillIssues", "backfillIssueComments"]
      min_lines: 150
    - path: "src/knowledge/issue-comment-chunker.ts"
      provides: "Comment chunking with issue context prefix and bot detection"
      exports: ["chunkIssueComment", "buildIssueEmbeddingText", "buildCommentEmbeddingText"]
    - path: "src/knowledge/issue-backfill.test.ts"
      provides: "Tests for backfill engine"
    - path: "src/knowledge/issue-comment-chunker.test.ts"
      provides: "Tests for comment chunker"
  key_links:
    - from: "src/knowledge/issue-backfill.ts"
      to: "src/knowledge/issue-store.ts"
      via: "IssueStore.upsert() and upsertComment() calls"
      pattern: "store\\.upsert|store\\.upsertComment"
    - from: "src/knowledge/issue-backfill.ts"
      to: "src/knowledge/embeddings.ts"
      via: "EmbeddingProvider.generate() for issue vectors"
      pattern: "embeddingProvider\\.generate"
    - from: "src/knowledge/issue-backfill.ts"
      to: "src/db/migrations/015-issue-sync-state.sql"
      via: "SQL queries against issue_sync_state table"
      pattern: "issue_sync_state"
---

<objective>
Build the issue backfill engine: migration for sync state tracking, comment chunker with issue context prefix, and the core backfill function that paginates GitHub Issues API, filters PRs, embeds issues, and persists sync state for resume.

Purpose: Provides the reusable engine that both the CLI script (Plan 02) and nightly sync invoke.
Output: Migration 015, issue-backfill.ts, issue-comment-chunker.ts, and their tests.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/106-historical-corpus-population/106-CONTEXT.md
@.planning/phases/106-historical-corpus-population/106-RESEARCH.md

@src/knowledge/review-comment-backfill.ts
@src/knowledge/review-comment-chunker.ts
@src/knowledge/issue-store.ts
@src/knowledge/issue-types.ts
@src/knowledge/embeddings.ts
@scripts/backfill-review-comments.ts
@src/db/migrations/005-review-comments.sql

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/knowledge/issue-types.ts:
```typescript
export type IssueInput = {
  repo: string; owner: string; issueNumber: number; title: string;
  body: string | null; state: string; authorLogin: string;
  authorAssociation: string | null; labelNames: string[];
  templateSlug: string | null; commentCount: number;
  assignees: Array<{ id: number; login: string }>; milestone: string | null;
  reactionCount: number; isPullRequest: boolean; locked: boolean;
  githubCreatedAt: Date; githubUpdatedAt: Date | null; closedAt: Date | null;
  embedding?: Float32Array | null;
};

export type IssueCommentInput = {
  repo: string; issueNumber: number; commentGithubId: number;
  authorLogin: string; authorAssociation: string | null; body: string;
  githubCreatedAt: Date; githubUpdatedAt: Date | null;
  embedding?: Float32Array | null;
};

export type IssueStore = {
  upsert(issue: IssueInput): Promise<void>;
  upsertComment(comment: IssueCommentInput): Promise<void>;
  getByNumber(repo: string, issueNumber: number): Promise<IssueRecord | null>;
  countByRepo(repo: string): Promise<number>;
  // ... other methods
};
```

From src/knowledge/embeddings.ts:
```typescript
export function createEmbeddingProvider(opts: {
  apiKey: string; logger: Logger; model?: string;
}): EmbeddingProvider;
// EmbeddingProvider.generate(text: string): Promise<Float32Array | null>
```

From src/knowledge/review-comment-backfill.ts (pattern to follow):
```typescript
export type BackfillOptions = {
  octokit: Octokit; store: ReviewCommentStore;
  embeddingProvider: EmbeddingProvider; repo: string;
  monthsBack?: number; botLogins?: Set<string>;
  logger: Logger; dryRun?: boolean;
};
export type BackfillResult = {
  totalComments: number; totalChunks: number;
  totalEmbeddings: number; pagesProcessed: number;
  durationMs: number; resumed: boolean;
};
```

From src/knowledge/review-comment-chunker.ts:
```typescript
export function countTokens(text: string): number;  // whitespace split
export function chunkReviewThread(...): ReviewCommentChunk[];
```

Existing sync state pattern (from migration 005):
```sql
CREATE TABLE IF NOT EXISTS review_comment_sync_state (
  id SERIAL PRIMARY KEY,
  repo TEXT NOT NULL UNIQUE,
  last_synced_at TIMESTAMPTZ,
  last_page_cursor TEXT,
  total_synced INTEGER NOT NULL DEFAULT 0,
  backfill_complete BOOLEAN NOT NULL DEFAULT false,
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migration and comment chunker</name>
  <files>
    src/db/migrations/015-issue-sync-state.sql
    src/db/migrations/015-issue-sync-state.down.sql
    src/knowledge/issue-comment-chunker.ts
    src/knowledge/issue-comment-chunker.test.ts
  </files>
  <action>
**Migration 015-issue-sync-state.sql:**
Create the `issue_sync_state` table following the exact pattern from `review_comment_sync_state` in migration 005. Schema:
```sql
CREATE TABLE IF NOT EXISTS issue_sync_state (
  id SERIAL PRIMARY KEY,
  repo TEXT NOT NULL UNIQUE,
  last_synced_at TIMESTAMPTZ,
  last_page_cursor TEXT,
  total_issues_synced INTEGER NOT NULL DEFAULT 0,
  total_comments_synced INTEGER NOT NULL DEFAULT 0,
  backfill_complete BOOLEAN NOT NULL DEFAULT false,
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
```
Down migration: `DROP TABLE IF EXISTS issue_sync_state;`

**issue-comment-chunker.ts:**
Create with these exports:
1. `buildIssueEmbeddingText(title: string, body: string | null): string` -- Returns `title` if no body, else `title\n\nbody`. Per user decision: embed full title + body (body trimming deferred per research Open Question 2).
2. `buildCommentEmbeddingText(issueNumber: number, issueTitle: string, commentBody: string): string` -- Returns `Issue #N: title\n\ncommentBody`. Per locked decision: prefix with parent issue context.
3. `chunkIssueComment(issueNumber: number, issueTitle: string, commentBody: string, opts?: { maxTokens?: number; overlap?: number }): string[]` -- For long comments, chunk with sliding window (default 1024 tokens, 256 overlap). Each chunk gets the issue context prefix. Use `countTokens()` from review-comment-chunker.ts (whitespace split). Short comments return a single-element array.
4. `isBotComment(login: string, botLogins?: Set<string>): boolean` -- Default bot set: dependabot, renovate, kodiai, github-actions, codecov, stale, kodi-butler. Also matches logins ending with `[bot]`.

**issue-comment-chunker.test.ts:**
Test: buildIssueEmbeddingText with/without body. buildCommentEmbeddingText produces correct prefix. chunkIssueComment returns single chunk for short comments, multiple chunks with overlap for long comments. isBotComment detects known bots, [bot] suffix, and allows human logins.
  </action>
  <verify>bun test src/knowledge/issue-comment-chunker.test.ts</verify>
  <done>Migration file exists. Comment chunker exports all 4 functions. All chunker tests pass. Bot detection matches kodi-butler and [bot] suffix logins.</done>
</task>

<task type="auto">
  <name>Task 2: Core backfill engine</name>
  <files>
    src/knowledge/issue-backfill.ts
    src/knowledge/issue-backfill.test.ts
  </files>
  <action>
**issue-backfill.ts:**
Follow `review-comment-backfill.ts` structure exactly. Create these types and functions:

**Types:**
```typescript
type IssueSyncState = {
  repo: string;
  lastSyncedAt: Date | null;
  lastPageCursor: string | null;
  totalIssuesSynced: number;
  totalCommentsSynced: number;
  backfillComplete: boolean;
};

type IssueBackfillOptions = {
  octokit: Octokit;
  store: IssueStore;
  sql: Sql;  // for sync state queries
  embeddingProvider: EmbeddingProvider;
  repo: string;  // "owner/repo"
  dryRun?: boolean;
  logger: Logger;
};

type IssueBackfillResult = {
  totalIssues: number;
  totalComments: number;
  totalEmbeddings: number;
  pagesProcessed: number;
  failedEmbeddings: number;
  durationMs: number;
  resumed: boolean;
};
```

**Sync state helpers (private):**
- `getIssueSyncState(sql, repo)` -- SELECT from issue_sync_state, return null if no row
- `updateIssueSyncState(sql, state: IssueSyncState)` -- UPSERT into issue_sync_state (INSERT ON CONFLICT UPDATE)

**`backfillIssues(opts: IssueBackfillOptions): Promise<IssueBackfillResult>`:**
1. Check sync state for resume point. If `lastSyncedAt` exists, resume from there (log "Resuming from...").
2. Paginate `octokit.rest.issues.listForRepo({ owner, repo, state: "all", sort: "updated", direction: "asc", since: lastSyncedAt?.toISOString(), per_page: 100, page })`.
3. For each page:
   a. Filter out items where `item.pull_request` is truthy (INGEST-03).
   b. For each issue: build embedding text via `buildIssueEmbeddingText(title, body)`, call `embeddingProvider.generate()` (fail-open: log and skip on null), call `store.upsert()` with the IssueInput.
   c. After each page: call `updateIssueSyncState()` with current page, latest `updated_at`, and running totals (INGEST-04).
   d. Log structured progress: `{ page, issuesOnPage, totalProcessed, embeddingsCreated, rateLimitRemaining }` (INGEST-05).
   e. Call `adaptiveRateDelay()` and `waitForRateReset()` using response headers.
4. After all pages: mark `backfillComplete: true` in sync state.
5. Return result summary.

**`backfillIssueComments(opts: IssueBackfillOptions): Promise<{ totalComments: number; totalChunks: number; failedEmbeddings: number }>`:**
1. Use repo-wide endpoint: `octokit.rest.issues.listCommentsForRepo({ owner, repo, sort: "created", direction: "asc", since: lastSyncedAt?.toISOString(), per_page: 100, page })`.
2. For each comment:
   a. Extract issue number from `comment.issue_url` using regex `/\/issues\/(\d+)$/`.
   b. Skip if `isBotComment(comment.user.login)`.
   c. Build chunks via `chunkIssueComment()`. For each chunk: embed and upsert via `store.upsertComment()`. If comment has multiple chunks, upsert only the first chunk as the "primary" comment (since upsertComment uses commentGithubId as the unique key). Store additional chunks as separate comment records with synthetic IDs (commentGithubId * 1000 + chunkIndex) to avoid key collision.
   d. Adaptive rate delay after each page.
3. Update sync state with comment totals.

**Rate limiting (reuse from review-comment-backfill pattern):**
- `adaptiveRateDelay(headers, logger)` -- proportional delay based on remaining/limit ratio
- `waitForRateReset(headers, logger)` -- hard sleep until x-ratelimit-reset when remaining === 0 (per locked decision: sleep until reset, auto-resume)

**Important:** Need an in-memory `Map<number, string>` cache for issue number -> title mapping so comment embedding can prefix with issue context. Populate from issues already in DB via `store.getByNumber()` or build during issue backfill pass.

**issue-backfill.test.ts:**
Test with mocked Octokit, store, embedding provider, and sql:
1. backfillIssues filters out PRs (items with pull_request field)
2. backfillIssues persists sync state after each page
3. backfillIssues resumes from existing sync state
4. backfillIssues handles embedding failures (fail-open, increments failedEmbeddings)
5. backfillIssueComments skips bot comments
6. backfillIssueComments extracts issue number from issue_url
7. Rate delay logic respects low remaining budget
  </action>
  <verify>bun test src/knowledge/issue-backfill.test.ts</verify>
  <done>backfillIssues paginates, filters PRs, embeds, upserts, tracks sync state, and logs structured progress. backfillIssueComments uses repo-wide endpoint, filters bots, chunks long comments, embeds with issue context prefix. All tests pass.</done>
</task>

</tasks>

<verification>
1. `bun test src/knowledge/issue-comment-chunker.test.ts` -- all chunker tests pass
2. `bun test src/knowledge/issue-backfill.test.ts` -- all backfill engine tests pass
3. Migration file exists at src/db/migrations/015-issue-sync-state.sql with CREATE TABLE
4. No new dependencies added (all imports from existing project modules)
</verification>

<success_criteria>
- Migration 015 creates issue_sync_state table with correct schema
- issue-comment-chunker.ts exports buildIssueEmbeddingText, buildCommentEmbeddingText, chunkIssueComment, isBotComment
- issue-backfill.ts exports backfillIssues and backfillIssueComments
- Backfill filters PRs via pull_request field check
- Sync state persisted after each page for cursor-based resume
- Structured logging with page counts, embedding counts, rate limit remaining
- Bot comments filtered before embedding
- Long comments chunked with overlap, prefixed with issue context
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/106-historical-corpus-population/106-01-SUMMARY.md`
</output>
