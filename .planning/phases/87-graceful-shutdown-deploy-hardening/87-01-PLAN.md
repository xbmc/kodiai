---
phase: 87-graceful-shutdown-deploy-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/migrations/004-webhook-queue.sql
  - src/db/migrations/004-webhook-queue.down.sql
  - src/lifecycle/shutdown-manager.ts
  - src/lifecycle/request-tracker.ts
  - src/lifecycle/webhook-queue-store.ts
  - src/lifecycle/types.ts
  - src/index.ts
autonomous: true
requirements: [DEP-01, DEP-02, DEP-03]

must_haves:
  truths:
    - "SIGTERM causes the server to stop accepting new work and drain in-flight requests/jobs before exiting"
    - "Grace window defaults to 5 minutes and is configurable via SHUTDOWN_GRACE_MS"
    - "If grace window expires with work in-flight, it extends once (doubles), then force-exits with code 1"
    - "New webhooks arriving during drain are accepted and queued to PostgreSQL webhook_queue table"
    - "Force exit logs what was abandoned"
  artifacts:
    - path: "src/db/migrations/004-webhook-queue.sql"
      provides: "webhook_queue table for durable webhook queuing during drain"
      contains: "CREATE TABLE webhook_queue"
    - path: "src/lifecycle/shutdown-manager.ts"
      provides: "SIGTERM handler with drain logic and grace window extension"
      exports: ["createShutdownManager"]
    - path: "src/lifecycle/request-tracker.ts"
      provides: "In-flight HTTP request and background job tracking"
      exports: ["createRequestTracker"]
    - path: "src/lifecycle/webhook-queue-store.ts"
      provides: "PostgreSQL-backed webhook queue for durability across restarts"
      exports: ["createWebhookQueueStore"]
  key_links:
    - from: "src/index.ts"
      to: "src/lifecycle/shutdown-manager.ts"
      via: "process.on('SIGTERM') wired at startup"
      pattern: "process\\.on.*SIGTERM"
    - from: "src/lifecycle/shutdown-manager.ts"
      to: "src/lifecycle/request-tracker.ts"
      via: "shutdown manager queries tracker for in-flight counts"
      pattern: "tracker\\.(activeRequests|activeJobs)"
    - from: "src/routes/webhooks.ts"
      to: "src/lifecycle/webhook-queue-store.ts"
      via: "webhooks queued to PostgreSQL during drain instead of dispatching"
      pattern: "webhookQueueStore\\.enqueue"
---

<objective>
Implement SIGTERM handling, in-flight work tracking, drain logic with configurable grace window, and durable webhook queuing during shutdown.

Purpose: When the server receives SIGTERM (deploy or manual stop), it must drain in-flight HTTP requests and background jobs cleanly, queue any new incoming webhooks to PostgreSQL for replay after restart, and exit gracefully.

Output: Migration for webhook_queue table, shutdown manager, request tracker, webhook queue store, and wiring in index.ts.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/87-graceful-shutdown-deploy-hardening/87-CONTEXT.md

@src/index.ts
@src/db/client.ts
@src/db/migrate.ts
@src/db/migrations/001-initial-schema.sql
@src/jobs/queue.ts
@src/jobs/types.ts
@src/routes/webhooks.ts
@src/routes/slack-events.ts
@src/routes/health.ts
@src/telemetry/store.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create webhook_queue migration, lifecycle types, and webhook queue store</name>
  <files>
    src/db/migrations/004-webhook-queue.sql
    src/db/migrations/004-webhook-queue.down.sql
    src/lifecycle/types.ts
    src/lifecycle/webhook-queue-store.ts
  </files>
  <action>
    **Migration (004-webhook-queue.sql):**
    Create `webhook_queue` table with columns:
    - `id` BIGSERIAL PRIMARY KEY
    - `source` TEXT NOT NULL — 'github' or 'slack'
    - `delivery_id` TEXT — GitHub delivery ID (nullable for Slack)
    - `event_name` TEXT — GitHub event name (nullable for Slack)
    - `headers` JSONB NOT NULL — original request headers needed for replay
    - `body` TEXT NOT NULL — raw request body (preserved for signature re-verification)
    - `queued_at` TIMESTAMPTZ NOT NULL DEFAULT NOW()
    - `processed_at` TIMESTAMPTZ — null until replayed
    - `status` TEXT NOT NULL DEFAULT 'pending' — 'pending', 'processing', 'completed', 'failed'

    Add index: `CREATE INDEX idx_webhook_queue_pending ON webhook_queue (status, queued_at) WHERE status = 'pending'`

    Down migration drops the table.

    **Types (src/lifecycle/types.ts):**
    Define interfaces:
    - `RequestTracker` — track/untrack HTTP requests and background jobs, query counts, waitForDrain(timeoutMs)
    - `ShutdownManager` — start(), isShuttingDown(), onShutdown callback registration
    - `WebhookQueueEntry` — shape of a queued webhook row
    - `WebhookQueueStore` — enqueue(), dequeueAll(), markCompleted(), markFailed()

    **Webhook queue store (src/lifecycle/webhook-queue-store.ts):**
    Implement `createWebhookQueueStore({ sql, logger })`:
    - `enqueue(entry)`: INSERT INTO webhook_queue with source, delivery_id, event_name, headers (JSONB), body
    - `dequeuePending()`: SELECT ... WHERE status = 'pending' ORDER BY queued_at ASC, then UPDATE status = 'processing' for those IDs. Use a transaction.
    - `markCompleted(id)`: UPDATE status = 'completed', processed_at = NOW()
    - `markFailed(id, error)`: UPDATE status = 'failed'
    - Accept a `telemetryStore` dependency (imported type `TelemetryStore` from `src/telemetry/types.ts`, instance created via `createTelemetryStore` from `src/telemetry/store.ts`)
    - On `enqueue()`: call `telemetryStore.record({ repo: entry.source, eventType: 'webhook_queued', model: 'none', conclusion: 'queued', deliveryId: entry.delivery_id ?? undefined, sessionId: entry.id?.toString() })` after the INSERT succeeds. Fire-and-forget (do not await — enqueue must not block on telemetry). Wrap in a try/catch that logs a warning on failure.
    - On `dequeuePending()`: after the batch UPDATE to 'processing', call `telemetryStore.record(...)` once per dequeued row with `eventType: 'webhook_replayed'`, `conclusion: 'replayed'`, same `repo` and `deliveryId` pattern. Also fire-and-forget with try/catch.
    - Structured logging can be kept in addition, but the `telemetry_events` table write is mandatory per locked decision.
  </action>
  <verify>
    - `bun test` passes (existing tests unbroken)
    - `bunx tsc --noEmit` passes
    - Migration file exists and contains CREATE TABLE webhook_queue
    - `grep -r 'telemetryStore.record' src/lifecycle/webhook-queue-store.ts` shows calls on enqueue and dequeuePending
    - `grep -r 'webhook_queued\|webhook_replayed' src/lifecycle/webhook-queue-store.ts` shows event types
  </verify>
  <done>
    webhook_queue migration ready to apply, WebhookQueueStore implemented with enqueue/dequeue/mark operations, lifecycle types defined, and enqueue/dequeuePending both write rows to telemetry_events via telemetryStore.record() per locked decision.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create request tracker and shutdown manager with SIGTERM wiring</name>
  <files>
    src/lifecycle/request-tracker.ts
    src/lifecycle/shutdown-manager.ts
    src/index.ts
    src/routes/webhooks.ts
    src/routes/slack-events.ts
  </files>
  <action>
    **Request tracker (src/lifecycle/request-tracker.ts):**
    Implement `createRequestTracker()` returning a `RequestTracker`:
    - Internal counters: `activeRequests` (HTTP handlers), `activeJobs` (background work like embedding generation, learning memory writes)
    - `trackRequest()`: increment activeRequests, return untrack function
    - `trackJob()`: increment activeJobs, return untrack function
    - `activeCount()`: returns { requests: number, jobs: number, total: number }
    - `waitForDrain(timeoutMs)`: returns a Promise that resolves when total === 0 or rejects after timeoutMs
    - waitForDrain polls every 500ms (simple interval, clears on resolution)

    **Shutdown manager (src/lifecycle/shutdown-manager.ts):**
    Implement `createShutdownManager({ logger, requestTracker, webhookQueueStore, closeDb, graceMs? })`:
    - `graceMs` defaults to `parseInt(process.env.SHUTDOWN_GRACE_MS) || 300_000` (5 minutes)
    - `isShuttingDown()`: returns boolean
    - `start()`: registers `process.on('SIGTERM', handler)` and `process.on('SIGINT', handler)`
    - SIGTERM handler:
      1. Log structured drain message: signal received, activeRequests, activeJobs, estimated drain time (graceMs)
      2. Set shutting down flag
      3. Try `requestTracker.waitForDrain(graceMs)`
      4. If drain succeeds: log success, close DB, exit 0
      5. If drain times out: log warning with remaining counts, EXTEND grace once (double graceMs), try waitForDrain again
      6. If extended drain times out: log error with what was abandoned (count of requests, jobs), close DB, exit 1
    - Guard against double-signal (if already shutting down, second SIGTERM logs "already shutting down" and returns)

    **Wire into index.ts:**
    1. Import and create `requestTracker` near the top (after DB setup)
    2. Import and create `webhookQueueStore` (passing sql, logger)
    3. Import and create `shutdownManager` (passing logger, requestTracker, webhookQueueStore, closeDb)
    4. Call `shutdownManager.start()` after server is configured

    **Wire request tracking into webhook routes:**
    - Pass `requestTracker` and `webhookQueueStore` and `shutdownManager` to `createWebhookRoutes`
    - In the GitHub webhook POST handler: if `shutdownManager.isShuttingDown()`, queue the webhook to PostgreSQL via `webhookQueueStore.enqueue()` instead of dispatching, return 200 with `{ received: true, queued: true }`
    - If NOT shutting down: wrap the existing fire-and-fork dispatch with `requestTracker.trackJob()` (the dispatch is async fire-and-forget, so track it as a background job)

    **Wire into Slack events route:**
    - Pass `shutdownManager` and `webhookQueueStore` to `createSlackEventRoutes`
    - If shutting down: queue the Slack event to webhook_queue with source='slack', return 200 immediately
    - If NOT shutting down: existing behavior, but wrap `onAllowedBootstrap` call with `requestTracker.trackJob()` to track the async work

    **IMPORTANT per user decision:** Readiness probe stays healthy during drain — single replica must keep accepting webhooks into the queue. Do NOT change the readiness route behavior here.
  </action>
  <verify>
    - `bun test` passes
    - `bunx tsc --noEmit` passes
    - `grep -r 'SIGTERM' src/lifecycle/shutdown-manager.ts` shows signal handler
    - `grep -r 'SHUTDOWN_GRACE_MS' src/lifecycle/shutdown-manager.ts` shows env var usage
    - `grep -r 'isShuttingDown' src/routes/webhooks.ts` shows drain-time queuing
  </verify>
  <done>
    SIGTERM handler installed, in-flight requests and background jobs tracked, grace window configurable via SHUTDOWN_GRACE_MS (default 5 min), webhooks queued to PostgreSQL during drain, double-signal guarded, force-exit with code 1 after extended grace logs abandoned work.
  </done>
</task>

</tasks>

<verification>
1. `bun test` — all existing tests pass
2. `bunx tsc --noEmit` — no type errors
3. Migration file contains CREATE TABLE webhook_queue with expected columns
4. SIGTERM handler registered in shutdown-manager.ts
5. SHUTDOWN_GRACE_MS env var read with 300000 default
6. Webhook routes queue to PostgreSQL when shutting down
7. Slack events route queues to PostgreSQL when shutting down
8. Request tracker tracks both HTTP requests and background jobs
9. Shutdown manager extends grace once on timeout, then force-exits with code 1
</verification>

<success_criteria>
- Server registers SIGTERM/SIGINT handlers at startup
- In-flight HTTP requests and background jobs are tracked and drained on shutdown
- Grace window defaults to 5 minutes, configurable via SHUTDOWN_GRACE_MS
- Grace extends once (doubles) if work remains, then force-exits code 1
- New webhooks during drain are durably queued to PostgreSQL webhook_queue table
- Force exit logs count of abandoned requests/jobs
</success_criteria>

<output>
After completion, create `.planning/phases/87-graceful-shutdown-deploy-hardening/87-01-SUMMARY.md`
</output>
