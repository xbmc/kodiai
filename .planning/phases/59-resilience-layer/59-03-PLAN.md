---
phase: 59-resilience-layer
plan: 03
type: execute
wave: 2
depends_on: ["59-01", "59-02"]
files_modified:
  - src/execution/mcp/index.ts
  - src/handlers/review.ts
  - src/execution/review-prompt.ts
autonomous: true

must_haves:
  truths:
    - "On timeout with checkpoint data having at least 1 finding, Kodiai publishes a partial review comment with disclaimer instead of a generic error"
    - "After publishing a partial review, Kodiai enqueues a retry job with reduced file scope focused on unreviewed files and halved timeout"
    - "When repo+author has 3+ recent timeouts, retry is skipped and the partial review explains why with splitting guidance"
    - "Retry result replaces the partial review comment via edit, producing a merged view of all analyzed files"
    - "Retry is capped at exactly 1 attempt -- no second retry regardless of outcome"
    - "Checkpoint MCP tool is only provided when timeout risk is medium or high"
    - "pr_author is recorded in telemetry for every review execution"
  artifacts:
    - path: "src/execution/mcp/index.ts"
      provides: "Checkpoint server wired into buildMcpServers when conditions met"
      contains: "createCheckpointServer"
    - path: "src/handlers/review.ts"
      provides: "Timeout path with partial publishing, retry enqueue, chronic timeout detection, and result merging"
      contains: "formatPartialReviewComment"
    - path: "src/execution/review-prompt.ts"
      provides: "Checkpoint instruction added to review prompt when checkpoint tool is available"
      contains: "save_review_checkpoint"
  key_links:
    - from: "src/handlers/review.ts"
      to: "src/knowledge/store.ts"
      via: "knowledgeStore.getCheckpoint() on timeout"
      pattern: "getCheckpoint"
    - from: "src/handlers/review.ts"
      to: "src/telemetry/store.ts"
      via: "telemetryStore.countRecentTimeouts() for chronic detection"
      pattern: "countRecentTimeouts"
    - from: "src/handlers/review.ts"
      to: "src/lib/partial-review-formatter.ts"
      via: "formatPartialReviewComment() for disclaimer"
      pattern: "formatPartialReviewComment"
    - from: "src/handlers/review.ts"
      to: "src/lib/retry-scope-reducer.ts"
      via: "computeRetryScope() for reduced file set"
      pattern: "computeRetryScope"
    - from: "src/handlers/review.ts"
      to: "src/jobs/types.ts"
      via: "jobQueue.enqueue() for retry job"
      pattern: "enqueue.*retry"
    - from: "src/execution/mcp/index.ts"
      to: "src/execution/mcp/checkpoint-server.ts"
      via: "createCheckpointServer() called in buildMcpServers"
      pattern: "createCheckpointServer"
---

<objective>
Wire checkpoint accumulation, partial review publishing, retry with scope reduction, and chronic timeout detection into the review execution pipeline.

Purpose: This is the integration plan that makes timeout resilience work end-to-end: the checkpoint MCP tool is conditionally provided to the executor, the review handler publishes partial results on timeout, retries with reduced scope when eligible, and skips retry for chronic timeout repo+author pairs.
Output: Modified review handler with complete timeout resilience path, MCP builder with conditional checkpoint server, review prompt with checkpoint instruction.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/59-resilience-layer/59-RESEARCH.md
@.planning/phases/59-resilience-layer/59-01-SUMMARY.md
@.planning/phases/59-resilience-layer/59-02-SUMMARY.md
@src/execution/mcp/index.ts
@src/execution/mcp/checkpoint-server.ts
@src/handlers/review.ts
@src/execution/review-prompt.ts
@src/execution/executor.ts
@src/execution/types.ts
@src/lib/partial-review-formatter.ts
@src/lib/retry-scope-reducer.ts
@src/lib/errors.ts
@src/lib/file-risk-scorer.ts
@src/telemetry/types.ts
@src/knowledge/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire checkpoint MCP server into executor and add prompt instruction</name>
  <files>src/execution/mcp/index.ts, src/execution/review-prompt.ts</files>
  <action>
**MCP builder** (`src/execution/mcp/index.ts`):
- Import `createCheckpointServer` from `./checkpoint-server.ts`.
- Import `KnowledgeStore` from `../../knowledge/types.ts`.
- Add optional parameters to `buildMcpServers` deps:
  - `knowledgeStore?: KnowledgeStore`
  - `totalFiles?: number`
  - `enableCheckpointTool?: boolean` (default false)
- When `enableCheckpointTool` is true AND `knowledgeStore` is defined AND `prNumber` is defined AND `reviewOutputKey` is defined:
  ```typescript
  if (deps.enableCheckpointTool && deps.knowledgeStore && deps.prNumber !== undefined && deps.reviewOutputKey) {
    servers.review_checkpoint = createCheckpointServer(
      deps.knowledgeStore,
      deps.reviewOutputKey,
      `${deps.owner}/${deps.repo}`,
      deps.prNumber,
      deps.totalFiles ?? 0,
      deps.logger,
    );
  }
  ```
- Re-export `createCheckpointServer` for direct use if needed.

**Review prompt** (`src/execution/review-prompt.ts`):
- Add an optional `checkpointEnabled?: boolean` parameter to `buildReviewPrompt`'s options/params (find the right place in its signature).
- When `checkpointEnabled` is true, append a checkpoint instruction section to the prompt:
  ```
  IMPORTANT: This review may time out. Call the save_review_checkpoint tool after reviewing every 3-5 files. Include:
  - filesReviewed: list of file paths you have fully analyzed
  - findingCount: total findings generated so far
  - summaryDraft: a brief summary of findings so far

  This ensures your work is preserved if the session times out.
  ```
- Only add this instruction when `checkpointEnabled` is explicitly true. This keeps it off for low-risk reviews.
  </action>
  <verify>Run `bunx tsc --noEmit` -- no type errors. Run `bun test src/execution/` -- existing executor and MCP tests pass. Verify checkpoint server is conditionally wired by grepping index.ts for `enableCheckpointTool`.</verify>
  <done>Checkpoint MCP server is conditionally wired into the MCP builder when enabled, and the review prompt includes checkpoint instructions when the tool is available.</done>
</task>

<task type="auto">
  <name>Task 2: Implement timeout resilience path in review handler</name>
  <files>src/handlers/review.ts</files>
  <action>
This is the core wiring task. Modify the review handler's timeout handling in `handleReview()`:

**Imports to add:**
- `formatPartialReviewComment` from `../lib/partial-review-formatter.ts`
- `computeRetryScope` from `../lib/retry-scope-reducer.ts`

**Pass checkpoint enablement to MCP builder:**
- Where `buildMcpServers` is called in the review handler, add:
  - `knowledgeStore: knowledgeStore` (already available in handler scope)
  - `totalFiles: changedFiles.length` (or the appropriate file count variable)
  - `enableCheckpointTool: (timeoutEstimate?.riskLevel === "medium" || timeoutEstimate?.riskLevel === "high")` -- only enable for medium/high risk
- Where `buildReviewPrompt` is called, pass `checkpointEnabled: (timeoutEstimate?.riskLevel === "medium" || timeoutEstimate?.riskLevel === "high")`.

**Record pr_author in telemetry:**
- In the telemetry `record()` call at the end of the handler, add `prAuthor: pr.user.login` to the TelemetryRecord.

**Replace the timeout error path** (the `if (result.conclusion === "error")` block around lines 2724-2763):
- Keep the existing non-timeout error handling unchanged.
- For timeout cases (`result.isTimeout === true`), replace the current error comment logic with:

```typescript
if (result.isTimeout) {
  // Step 1: Read checkpoint data
  const checkpoint = knowledgeStore?.getCheckpoint?.(reviewOutputKey) ?? null;
  const hasPublishedInlines = result.published ?? false;
  const hasPartialResults = (checkpoint?.findingCount ?? 0) >= 1 || hasPublishedInlines;

  if (hasPartialResults) {
    // Step 2: Check chronic timeout threshold before publishing
    const recentTimeouts = telemetryStore.countRecentTimeouts?.(
      `${apiOwner}/${apiRepo}`,
      pr.user.login,
    ) ?? 0;
    const isChronicTimeout = recentTimeouts >= 3;

    // Step 3: Publish partial review with disclaimer
    const timeoutDuration = timeoutEstimate?.dynamicTimeoutSeconds ?? config.timeoutSeconds;
    const partialBody = formatPartialReviewComment({
      summaryDraft: checkpoint?.summaryDraft ?? "Review timed out with findings posted inline above.",
      filesReviewed: checkpoint?.filesReviewed?.length ?? 0,
      totalFiles: changedFiles.length,
      timedOutAfterSeconds: timeoutDuration,
      isRetrySkipped: isChronicTimeout,
      retrySkipReason: isChronicTimeout
        ? "Retry skipped -- this repo has timed out frequently for this author."
        : undefined,
    });

    const octokit = await githubApp.getInstallationOctokit(event.installationId);
    const partialComment = await octokit.rest.issues.createComment({
      owner: apiOwner,
      repo: apiRepo,
      issue_number: pr.number,
      body: sanitizeOutgoingMentions(partialBody, [githubApp.getAppSlug(), "claude"]),
    });
    const partialCommentId = partialComment.data.id;

    // Store partial comment ID in checkpoint for retry to find
    knowledgeStore?.updateCheckpointCommentId?.(reviewOutputKey, partialCommentId);

    logger.info({
      deliveryId: event.id,
      prNumber: pr.number,
      partialCommentId,
      filesReviewed: checkpoint?.filesReviewed?.length ?? 0,
      findingCount: checkpoint?.findingCount ?? 0,
      isChronicTimeout,
      recentTimeouts,
    }, "Published partial review on timeout");

    // Step 4: Enqueue retry if eligible (not chronic, exactly 1 retry)
    if (!isChronicTimeout) {
      const retryReviewOutputKey = `${reviewOutputKey}-retry-1`;
      const retryTimeout = Math.max(30, Math.floor(timeoutDuration / 2));

      // Compute retry scope
      const filesAlreadyReviewed = checkpoint?.filesReviewed ?? [];
      // fileRiskScores should be available from earlier in the handler
      const retryScope = computeRetryScope({
        allFiles: fileRiskScores,
        filesAlreadyReviewed,
        totalFiles: changedFiles.length,
      });

      if (retryScope.filesToReview.length > 0) {
        logger.info({
          deliveryId: event.id,
          prNumber: pr.number,
          retryFiles: retryScope.filesToReview.length,
          scopeRatio: retryScope.scopeRatio,
          retryTimeout,
        }, "Enqueueing retry with reduced scope");

        // Fire-and-forget enqueue -- do not await the retry result
        jobQueue.enqueue(event.installationId, async () => {
          try {
            // Build reduced-scope retry
            const retryChangedFiles = retryScope.filesToReview.map(f => f.filePath);

            // Create a new workspace for retry
            const retryWorkspace = await workspaceManager.create(event.installationId, {
              owner: apiOwner,
              repo: apiRepo,
              ref: pr.head.ref,
              depth: 50,
            });

            try {
              // Checkout PR head
              await fetchAndCheckoutPullRequestHeadRef(retryWorkspace, pr, logger);

              // Build retry prompt with reduced file set
              // Re-use the same config, enrichment, etc. but with reduced files
              const retryPrompt = buildReviewPrompt({
                // ... same params as original but with retryChangedFiles instead of changedFiles
                // and a note that this is a retry with reduced scope
                // The executor will read the prompt and review only the specified files
                ...reviewPromptParams,
                changedFiles: retryChangedFiles,
                checkpointEnabled: false, // No checkpoint on retry
                retryContext: {
                  isRetry: true,
                  originalFilesReviewed: filesAlreadyReviewed.length,
                  totalFiles: changedFiles.length,
                },
              });

              const retryResult = await executor.execute({
                workspace: retryWorkspace,
                installationId: event.installationId,
                owner: apiOwner,
                repo: apiRepo,
                prNumber: pr.number,
                commentId: undefined,
                eventType: "pull_request.review-retry",
                triggerBody: "",
                prompt: retryPrompt,
                reviewOutputKey: retryReviewOutputKey,
                deliveryId: `${event.id}-retry-1`,
                dynamicTimeoutSeconds: retryTimeout,
                botHandles: [githubApp.getAppSlug(), "claude"],
              });

              // Step 5: Merge results -- edit the partial review comment
              const retryCheckpoint = knowledgeStore?.getCheckpoint?.(retryReviewOutputKey) ?? null;
              const retryOctokit = await githubApp.getInstallationOctokit(event.installationId);

              if (retryResult.conclusion === "success" || (retryResult.isTimeout && ((retryCheckpoint?.findingCount ?? 0) >= 1 || retryResult.published))) {
                // Retry produced results -- merge and replace partial comment
                const retryFilesReviewed = retryCheckpoint?.filesReviewed?.length ?? retryScope.filesToReview.length;
                const mergedBody = formatPartialReviewComment({
                  summaryDraft: retryCheckpoint?.summaryDraft ?? checkpoint?.summaryDraft ?? "Review completed with reduced scope.",
                  filesReviewed: checkpoint?.filesReviewed?.length ?? 0,
                  totalFiles: changedFiles.length,
                  timedOutAfterSeconds: timeoutDuration,
                  isRetryResult: true,
                  retryFilesReviewed: retryFilesReviewed,
                });

                await retryOctokit.rest.issues.updateComment({
                  owner: apiOwner,
                  repo: apiRepo,
                  comment_id: partialCommentId,
                  body: sanitizeOutgoingMentions(mergedBody, [githubApp.getAppSlug(), "claude"]),
                });

                logger.info({
                  deliveryId: `${event.id}-retry-1`,
                  prNumber: pr.number,
                  retryConclusion: retryResult.conclusion,
                  retryFilesReviewed,
                  partialCommentId,
                }, "Retry complete -- updated partial review comment with merged results");
              } else {
                // Retry failed with no results -- keep original partial review as-is
                logger.info({
                  deliveryId: `${event.id}-retry-1`,
                  prNumber: pr.number,
                  retryConclusion: retryResult.conclusion,
                }, "Retry produced no additional results -- keeping original partial review");
              }

              // Record retry telemetry
              telemetryStore.record({
                deliveryId: `${event.id}-retry-1`,
                repo: `${apiOwner}/${apiRepo}`,
                prNumber: pr.number,
                eventType: "pull_request.review-retry",
                model: retryResult.model ?? "unknown",
                inputTokens: retryResult.inputTokens,
                outputTokens: retryResult.outputTokens,
                cacheReadTokens: retryResult.cacheReadTokens,
                cacheCreationTokens: retryResult.cacheCreationTokens,
                durationMs: retryResult.durationMs,
                costUsd: retryResult.costUsd,
                conclusion: retryResult.isTimeout ? "timeout" : retryResult.conclusion,
                sessionId: retryResult.sessionId,
                numTurns: retryResult.numTurns,
                stopReason: retryResult.stopReason,
                prAuthor: pr.user.login,
              });

              // Cleanup checkpoint data
              knowledgeStore?.deleteCheckpoint?.(reviewOutputKey);
              knowledgeStore?.deleteCheckpoint?.(retryReviewOutputKey);
            } finally {
              await retryWorkspace.cleanup();
            }
          } catch (retryErr) {
            logger.error({ err: retryErr, deliveryId: `${event.id}-retry-1`, prNumber: pr.number }, "Retry failed with error");
          }
        }, {
          deliveryId: `${event.id}-retry-1`,
          eventName: event.name,
          action: `review-retry`,
          jobType: "pull-request-review-retry",
          prNumber: pr.number,
        }).catch((err) => {
          logger.error({ err, deliveryId: event.id, prNumber: pr.number }, "Failed to enqueue retry job");
        });
      }
    }
  } else {
    // No partial results -- post standard timeout error comment (existing behavior)
    // ... keep existing timeout error comment logic ...
  }
  // After the timeout-specific handling, skip the normal error comment path for timeouts
}
```

**IMPORTANT implementation notes:**
1. The `fileRiskScores` variable must be available in the timeout path. It should already be computed earlier in the handler (from `computeFileRiskScores`). If it's scoped inside a block, hoist its declaration to be accessible in the timeout path.
2. The `reviewPromptParams` placeholder represents the same params used for the original buildReviewPrompt call. Extract these into a variable earlier in the handler so they can be reused for retry.
3. The retry `buildReviewPrompt` call needs to pass `retryChangedFiles` instead of the full `changedFiles`. If `buildReviewPrompt` doesn't accept a `retryContext` param yet, add it (or just prepend a "This is a retry of a timed-out review. Focus on the files listed below." note to the prompt string).
4. The retry job is fire-and-forget: the original handler returns immediately after enqueueing. The retry runs in the job queue respecting per-installation concurrency.
5. Do NOT use `await` on the `jobQueue.enqueue()` call for the retry -- it should be fire-and-forget with a `.catch()` for error logging.
6. Keep the existing non-timeout error handling (`else if (category === "timeout")`, `else { ... }`) for the case where `result.isTimeout` is true but `hasPartialResults` is false.
  </action>
  <verify>Run `bunx tsc --noEmit` -- no type errors. Run `bun test src/handlers/ --timeout 30000` -- existing review handler tests still pass. Run `bun test` -- full test suite passes.</verify>
  <done>Review handler has complete timeout resilience: publishes partial reviews with disclaimers on timeout, enqueues retry with adaptive scope reduction for non-chronic timeouts, replaces partial comment with merged results on retry success, records pr_author in telemetry, and checkpoint MCP tool is conditionally enabled for medium/high risk reviews.</done>
</task>

</tasks>

<verification>
- `bun test` -- all tests pass (existing + new from plans 01 and 02)
- `bunx tsc --noEmit` -- no type errors
- Checkpoint MCP tool conditionally wired in buildMcpServers (only when enableCheckpointTool is true)
- Review prompt includes checkpoint instruction only for medium/high risk
- Timeout path publishes partial review instead of generic error when checkpoint data has findings
- Retry enqueued via jobQueue with reduced scope and halved timeout
- Chronic timeout detection skips retry for repo+author with 3+ timeouts in 7 days
- Retry replaces partial review comment via updateComment
- pr_author recorded in telemetry for all review executions
</verification>

<success_criteria>
End-to-end timeout resilience works: when a review times out, Kodiai publishes whatever was completed as a partial review with a clear disclaimer, optionally retries with a reduced file scope focused on unreviewed files, and skips retry for chronically timing-out repo+author pairs. All per the locked user decisions.
</success_criteria>

<output>
After completion, create `.planning/phases/59-resilience-layer/59-03-SUMMARY.md`
</output>
