---
phase: 89-pr-review-comment-ingestion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/migrations/005-review-comments.sql
  - src/db/migrations/005-review-comments.down.sql
  - src/knowledge/review-comment-store.ts
  - src/knowledge/review-comment-store.test.ts
  - src/knowledge/review-comment-types.ts
  - src/knowledge/review-comment-chunker.ts
  - src/knowledge/review-comment-chunker.test.ts
autonomous: true
requirements: [KI-01, KI-02, KI-03]

must_haves:
  truths:
    - "knowledge.review_comments table exists in PostgreSQL with pgvector embedding column and all required metadata columns"
    - "Review comments can be stored with full metadata: repo, PR number, file path, line range, author, date, and thread grouping"
    - "Thread-aware chunking groups reply chains into single chunks when under token limit, splits with overlapping windows when over"
    - "HNSW index on review_comments.embedding enables cosine similarity search"
    - "Multi-repo support: repo column scopes all queries, enabling future cross-repo retrieval"
  artifacts:
    - path: "src/db/migrations/005-review-comments.sql"
      provides: "knowledge.review_comments table with pgvector embedding column, metadata columns, indexes"
      contains: "review_comments"
    - path: "src/knowledge/review-comment-store.ts"
      provides: "CRUD operations for review comment chunks with vector storage and retrieval"
      contains: "createReviewCommentStore"
    - path: "src/knowledge/review-comment-types.ts"
      provides: "Type definitions for review comment records, chunks, and store interface"
      contains: "ReviewCommentChunk"
    - path: "src/knowledge/review-comment-chunker.ts"
      provides: "Thread-aware semantic chunking with 1024-token windows and 256-token overlap"
      contains: "chunkReviewThread"
  key_links:
    - from: "src/knowledge/review-comment-store.ts"
      to: "src/db/client.ts"
      via: "Uses shared Sql connection pool from createDbClient"
      pattern: "Sql|sql`"
    - from: "src/knowledge/review-comment-store.ts"
      to: "src/knowledge/review-comment-types.ts"
      via: "Store implements ReviewCommentStore interface"
      pattern: "ReviewCommentStore|ReviewCommentChunk"
    - from: "src/db/migrations/005-review-comments.sql"
      to: "src/db/migrations/002-pgvector-indexes.sql"
      via: "HNSW index on embedding column uses same vector_cosine_ops pattern"
      pattern: "hnsw|vector_cosine_ops"
---

<objective>
Create the PostgreSQL schema, store module, and chunking logic for PR review comment ingestion.

Purpose: Establish the data layer that backfill (Plan 02), incremental sync (Plan 03), and retrieval (Plan 04) all depend on. Schema must support full metadata, vector search, and multi-repo scoping.
Output: Migration file, store module with write/read/search operations, type definitions, and thread-aware chunker.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@src/db/migrations/001-initial-schema.sql
@src/db/migrations/002-pgvector-indexes.sql
@src/db/migrate.ts
@src/db/client.ts
@src/knowledge/memory-store.ts
@src/knowledge/types.ts
@src/knowledge/embeddings.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create review_comments schema migration and type definitions</name>
  <files>src/db/migrations/005-review-comments.sql, src/db/migrations/005-review-comments.down.sql, src/knowledge/review-comment-types.ts</files>
  <action>
Create migration `005-review-comments.sql` with the `review_comments` table:

```sql
CREATE TABLE IF NOT EXISTS review_comments (
  id BIGSERIAL PRIMARY KEY,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),

  -- Source identity
  repo TEXT NOT NULL,              -- e.g. "xbmc/xbmc" (multi-repo ready)
  owner TEXT NOT NULL,             -- e.g. "xbmc" (for owner-level shared retrieval)
  pr_number INTEGER NOT NULL,
  pr_title TEXT,                   -- PR title for retrieval context
  comment_github_id BIGINT NOT NULL, -- GitHub comment ID (for edit/delete tracking)

  -- Thread grouping
  thread_id TEXT NOT NULL,         -- Grouping key: "{repo}:{pr}:{path}:{original_position}" or "{repo}:{pr}:{review_id}"
  in_reply_to_id BIGINT,          -- GitHub ID of parent comment (NULL if thread root)

  -- Location metadata
  file_path TEXT,                  -- File being reviewed (NULL for PR-level comments)
  start_line INTEGER,              -- Diff line start (NULL for PR-level)
  end_line INTEGER,                -- Diff line end
  diff_hunk TEXT,                  -- Surrounding diff context from GitHub API

  -- Author metadata
  author_login TEXT NOT NULL,
  author_association TEXT,         -- OWNER, MEMBER, CONTRIBUTOR, etc.

  -- Content
  body TEXT NOT NULL,              -- Raw markdown comment body
  chunk_index INTEGER NOT NULL DEFAULT 0,  -- 0 for single-chunk, 0..N for split threads
  chunk_text TEXT NOT NULL,        -- Chunked text for this embedding
  token_count INTEGER NOT NULL,

  -- Embedding
  embedding vector(1024),          -- voyage-code-3 embedding (same as learning_memories)
  embedding_model TEXT,
  stale BOOLEAN NOT NULL DEFAULT false,

  -- Lifecycle
  github_created_at TIMESTAMPTZ NOT NULL,
  github_updated_at TIMESTAMPTZ,
  deleted BOOLEAN NOT NULL DEFAULT false,  -- Soft delete for webhook delete events

  -- Sync tracking
  backfill_batch TEXT,             -- Batch identifier for resume tracking

  UNIQUE(repo, comment_github_id, chunk_index)
);
```

Add indexes:
- `idx_review_comments_repo` on `(repo)` — partition-like scoping
- `idx_review_comments_thread` on `(thread_id)` — thread grouping queries
- `idx_review_comments_pr` on `(repo, pr_number)` — per-PR lookups
- `idx_review_comments_author` on `(author_login)` — author filtering
- `idx_review_comments_github_id` on `(repo, comment_github_id)` — edit/delete lookups
- `idx_review_comments_embedding_hnsw` using HNSW on `(embedding vector_cosine_ops)` with `(m = 16, ef_construction = 64)` — same tuning as learning_memories
- `idx_review_comments_stale` on `(stale)` WHERE `stale = false` — active-only partial index
- tsvector GIN index on `chunk_text` for BM25-style full-text search (follow pattern from 003-tsvector-columns.sql)

Add a `review_comment_sync_state` table for cursor-based resume:
```sql
CREATE TABLE IF NOT EXISTS review_comment_sync_state (
  id SERIAL PRIMARY KEY,
  repo TEXT NOT NULL UNIQUE,
  last_synced_at TIMESTAMPTZ,       -- Timestamp of most recent comment processed
  last_page_cursor TEXT,             -- GitHub API cursor for resume
  total_comments_synced INTEGER NOT NULL DEFAULT 0,
  backfill_complete BOOLEAN NOT NULL DEFAULT false,
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
```

Create the `.down.sql` rollback file dropping both tables and indexes.

Create `src/knowledge/review-comment-types.ts` with:
- `ReviewCommentRecord` — full DB row type with all columns
- `ReviewCommentChunk` — the unit stored/embedded (chunk_text + metadata)
- `ReviewCommentInput` — raw GitHub API response fields before chunking
- `ReviewCommentStore` interface with methods:
  - `writeChunks(chunks: ReviewCommentChunk[]): Promise<void>` — bulk upsert
  - `softDelete(repo: string, commentGithubId: number): Promise<void>`
  - `updateChunks(chunks: ReviewCommentChunk[]): Promise<void>` — re-embed on edit
  - `searchByEmbedding(params: { queryEmbedding: Float32Array; repo: string; topK: number }): Promise<ReviewCommentSearchResult[]>`
  - `getThreadComments(threadId: string): Promise<ReviewCommentRecord[]>`
  - `getSyncState(repo: string): Promise<SyncState | null>`
  - `updateSyncState(state: SyncState): Promise<void>`
  - `getLatestCommentDate(repo: string): Promise<Date | null>` — for incremental sync
  - `countByRepo(repo: string): Promise<number>` — stats
- `ReviewCommentSearchResult` — search result with distance score
- `SyncState` — sync tracking record type

Design for multi-repo from the start: every query scopes by `repo` column. Store `owner` for future owner-level shared pool retrieval (Phase 91).
  </action>
  <verify>
- `bun run src/db/migrate.ts up` applies the migration without error
- TypeScript compiles: `bun build --no-bundle src/knowledge/review-comment-types.ts`
- Type definitions export all required interfaces
  </verify>
  <done>
- review_comments table exists with all metadata columns, embedding column, and indexes
- review_comment_sync_state table exists for cursor-based resume
- Down migration cleanly drops both tables
- All type definitions and interfaces exported
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement review comment store and thread-aware chunker</name>
  <files>src/knowledge/review-comment-store.ts, src/knowledge/review-comment-store.test.ts, src/knowledge/review-comment-chunker.ts, src/knowledge/review-comment-chunker.test.ts</files>
  <action>
**Chunker (`review-comment-chunker.ts`):**

Implement `chunkReviewThread()` with the user's locked decision: 1024-token windows, 256-token overlap.

Algorithm:
1. Accept an array of `ReviewCommentInput` records belonging to the same thread, ordered by `github_created_at`
2. Concatenate into a single thread text with author attribution: `@author (date): body` per comment, separated by newlines
3. Count tokens using a simple whitespace-based approximation (split on `/\s+/`, count — no external tokenizer dependency). Add a `countTokens(text: string): number` helper.
4. If total tokens <= 1024: produce a single chunk with `chunk_index = 0`
5. If total tokens > 1024: slide a window of 1024 tokens with 256-token overlap, producing chunks with `chunk_index = 0, 1, 2, ...`
6. Each chunk carries the full metadata from the thread root comment (repo, pr_number, file_path, line range, author of root)
7. `thread_id` generation: `{repo}:{pr_number}:{file_path}:{original_position}` for file-level comments, `{repo}:{pr_number}:general:{review_id}` for PR-level

Bot filtering (per user decision): accept a `botLogins: Set<string>` parameter. Skip entire threads where the PR author matches bot patterns. Filter individual comments by bots but keep the thread if human comments exist. Default bot patterns: `dependabot`, `renovate`, `kodiai`, `github-actions`, `codecov`, accounts with `[bot]` suffix.

Export: `chunkReviewThread(thread: ReviewCommentInput[], opts: { botLogins?: Set<string> }): ReviewCommentChunk[]`

**Store (`review-comment-store.ts`):**

Implement `createReviewCommentStore({ sql, logger })` returning `ReviewCommentStore`. Follow the same factory pattern as `createLearningMemoryStore` and `createKnowledgeStore`.

Key implementation details:
- `writeChunks`: Batch INSERT with `ON CONFLICT (repo, comment_github_id, chunk_index) DO NOTHING` for idempotent backfill
- `softDelete`: UPDATE `deleted = true` WHERE `repo = $1 AND comment_github_id = $2`
- `updateChunks`: DELETE existing chunks for the comment, then INSERT new chunks (handles re-chunking on edit)
- `searchByEmbedding`: Use `embedding <=> $queryVector::vector` for cosine distance, filter by `repo`, `stale = false`, `deleted = false`, ORDER BY distance, LIMIT topK. Return `ReviewCommentSearchResult` with distance, chunk text, and all metadata.
- `getSyncState`/`updateSyncState`: CRUD on `review_comment_sync_state`
- `getLatestCommentDate`: `SELECT MAX(github_created_at) FROM review_comments WHERE repo = $1 AND deleted = false`
- `countByRepo`: Simple COUNT

Use the same `float32ArrayToVectorString()` helper pattern from `memory-store.ts` for embedding serialization.

**Tests:**

For the chunker:
- Single short comment produces one chunk
- Thread of 3 short comments concatenated into one chunk
- Long thread (>1024 tokens) produces multiple overlapping chunks
- Bot comments are filtered out
- Thread with only bot comments produces zero chunks
- Empty thread produces zero chunks
- Thread ID generation for file-level vs PR-level comments

For the store (needs PostgreSQL — use `beforeAll`/`afterAll` with real DB connection like `memory-store.test.ts`):
- writeChunks stores and retrieves by thread
- writeChunks is idempotent (re-run does not duplicate)
- softDelete marks comment as deleted
- updateChunks replaces existing chunks
- searchByEmbedding returns results sorted by distance
- getSyncState/updateSyncState round-trips
- countByRepo returns correct count
  </action>
  <verify>
- `bun test src/knowledge/review-comment-chunker.test.ts` passes
- `bun test src/knowledge/review-comment-store.test.ts` passes (requires DATABASE_URL)
- All existing tests still pass: `bun test`
  </verify>
  <done>
- Chunker correctly handles single, multi-comment, and oversized threads with 1024/256 windowing
- Bot filtering excludes bot-authored content per decision
- Store implements full ReviewCommentStore interface with vector search
- All tests pass including existing suite
  </done>
</task>

</tasks>

<verification>
- Migration 005 applies and rolls back cleanly
- `review_comments` table has embedding column, HNSW index, tsvector GIN index, and all metadata columns
- `review_comment_sync_state` table supports cursor-based resume
- Chunker produces correct token windows with 1024/256 overlap
- Store write/read/search operations work with real PostgreSQL
- All existing tests continue to pass
</verification>

<success_criteria>
- New migration applies without breaking existing schema
- Store can write, read, search, and soft-delete review comment chunks
- Chunker handles thread concatenation and sliding-window splitting
- Bot filtering works with configurable login set
- Type definitions cover all required interfaces
</success_criteria>

<output>
After completion, create `.planning/phases/89-pr-review-comment-ingestion/89-01-SUMMARY.md`
</output>
