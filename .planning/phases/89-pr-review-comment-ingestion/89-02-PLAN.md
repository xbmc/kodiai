---
phase: 89-pr-review-comment-ingestion
plan: 02
type: execute
wave: 2
depends_on: [89-01]
files_modified:
  - src/knowledge/review-comment-backfill.ts
  - src/knowledge/review-comment-backfill.test.ts
  - scripts/backfill-review-comments.ts
  - src/knowledge/index.ts
  - package.json
autonomous: true
requirements: [KI-01, KI-02]

must_haves:
  truths:
    - "CLI command fetches all PR review comments from xbmc/xbmc for the past 18 months via GitHub API"
    - "Backfill is cursor-based resumable: re-running picks up where it left off using sync_state"
    - "GitHub API rate consumption is throttled to ~2500 req/hour (50% of authenticated limit) to leave room for normal operations"
    - "Verbose logging reports every batch with counts, PR numbers, and running totals"
    - "Bot-authored PRs are skipped during backfill per user decision"
    - "Each fetched comment is chunked, embedded via VoyageAI, and stored in review_comments table"
  artifacts:
    - path: "src/knowledge/review-comment-backfill.ts"
      provides: "Backfill engine with GitHub API pagination, rate limiting, chunking, and embedding pipeline"
      contains: "backfillReviewComments"
    - path: "scripts/backfill-review-comments.ts"
      provides: "CLI entry point: bun run backfill:reviews"
      contains: "backfill"
    - path: "src/knowledge/index.ts"
      provides: "Updated barrel exports including review comment store"
      contains: "createReviewCommentStore"
    - path: "package.json"
      provides: "npm script wiring for backfill CLI"
      contains: "backfill:reviews"
  key_links:
    - from: "src/knowledge/review-comment-backfill.ts"
      to: "src/knowledge/review-comment-store.ts"
      via: "Backfill engine writes chunks via ReviewCommentStore.writeChunks()"
      pattern: "writeChunks|ReviewCommentStore"
    - from: "src/knowledge/review-comment-backfill.ts"
      to: "src/knowledge/review-comment-chunker.ts"
      via: "Raw API responses are chunked before embedding"
      pattern: "chunkReviewThread"
    - from: "src/knowledge/review-comment-backfill.ts"
      to: "src/knowledge/embeddings.ts"
      via: "Each chunk is embedded via EmbeddingProvider.generate() before storage"
      pattern: "embeddingProvider|generate"
    - from: "scripts/backfill-review-comments.ts"
      to: "src/knowledge/review-comment-backfill.ts"
      via: "CLI wires dependencies and invokes backfill engine"
      pattern: "backfillReviewComments"
---

<objective>
Build the backfill CLI that fetches 18 months of PR review comments from xbmc/xbmc via GitHub API, chunks them, embeds them, and stores them in the review_comments table.

Purpose: Populate the review comment corpus with historical human review patterns that the bot can reference when reviewing new PRs.
Output: Backfill engine module and CLI script (`npm run backfill:reviews`).
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/89-pr-review-comment-ingestion/89-01-SUMMARY.md
@src/knowledge/review-comment-store.ts
@src/knowledge/review-comment-types.ts
@src/knowledge/review-comment-chunker.ts
@src/knowledge/embeddings.ts
@src/db/client.ts
@src/auth/github-app.ts
@src/knowledge/memory-store.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement backfill engine with GitHub API pagination and rate limiting</name>
  <files>src/knowledge/review-comment-backfill.ts, src/knowledge/review-comment-backfill.test.ts</files>
  <action>
Create `src/knowledge/review-comment-backfill.ts` with a `backfillReviewComments()` function.

**GitHub API strategy:**

Use `GET /repos/{owner}/{repo}/pulls/comments` endpoint (lists all review comments across all PRs, sorted by `created` or `updated`). This is more efficient than per-PR fetching.

Parameters:
- `sort=created`, `direction=asc`
- `since` parameter set to 18 months ago (or `sync_state.last_synced_at` for resume)
- `per_page=100` (max)
- Page through results using Link header pagination

**Rate limiting (locked decision: ~2500 req/hour):**

Implement a simple token-bucket style throttle:
- Track requests made in current window
- After each API call, check `x-ratelimit-remaining` header
- If remaining < 50% of limit, add a delay: `sleep(1500)` between requests
- If remaining < 20%, increase delay to `sleep(3000)`
- Log rate limit status every 10 pages

**Processing pipeline per page:**

1. Fetch page of 100 comments
2. Filter out comments by bot authors (check `user.login` against bot set and `user.type === 'Bot'`)
3. Group remaining comments by thread (using `pull_request_review_id` + `in_reply_to_id` to build threads)
4. For each thread group, call `chunkReviewThread()` to produce chunks
5. For each chunk, call `embeddingProvider.generate(chunk.chunk_text, "document")` to get embedding
6. If embedding returns null (fail-open), still store the chunk without embedding (set `embedding = null`)
7. Call `store.writeChunks(chunks)` to persist
8. Update sync state after each page: `last_synced_at`, `total_comments_synced`, `last_page_cursor`

**Thread assembly logic:**

GitHub's list-all-comments endpoint returns flat results. To group into threads:
- Comments with `in_reply_to_id` are replies — group with parent
- Comments without `in_reply_to_id` that share same `pull_request_review_id` are thread roots
- Standalone comments (no reply, unique review) are single-comment threads

**Resume logic:**

On startup:
1. Check `review_comment_sync_state` for this repo
2. If `backfill_complete = true`, log and exit
3. If `last_synced_at` exists, use as `since` parameter
4. Continue pagination from where we left off

**Verbose logging (locked decision):**

Every batch (page): `"Backfill batch processed" { page, commentsInBatch, threadCount, chunksProduced, embeddingsGenerated, embeddingsFailed, totalSoFar, rateRemaining }`

On completion: `"Backfill complete" { repo, totalComments, totalChunks, duration, pagesProcessed }`

**Function signature:**

```typescript
export async function backfillReviewComments(opts: {
  octokit: Octokit;
  store: ReviewCommentStore;
  embeddingProvider: EmbeddingProvider;
  repo: string;        // "xbmc/xbmc"
  monthsBack?: number; // default 18
  botLogins?: Set<string>;
  logger: Logger;
}): Promise<BackfillResult>
```

`BackfillResult`: `{ totalComments, totalChunks, totalEmbeddings, pagesProcessed, durationMs, resumed: boolean }`

**Manual PR re-sync (locked decision):**

Add a `syncSinglePR()` function for the CLI sub-command:

```typescript
export async function syncSinglePR(opts: {
  octokit: Octokit;
  store: ReviewCommentStore;
  embeddingProvider: EmbeddingProvider;
  repo: string;
  prNumber: number;
  botLogins?: Set<string>;
  logger: Logger;
}): Promise<{ chunksWritten: number }>
```

This fetches `GET /repos/{owner}/{repo}/pulls/{pr}/comments`, chunks, embeds, and upserts.

**Tests:**

Unit tests with mocked Octokit and store:
- Processes pages of comments and stores chunks
- Resumes from sync state
- Skips bot comments
- Handles rate limit headers (adds delay when low)
- Groups comments into threads correctly
- Handles empty pages (end of pagination)
- syncSinglePR fetches and processes one PR
  </action>
  <verify>
- `bun test src/knowledge/review-comment-backfill.test.ts` passes
- All existing tests pass: `bun test`
  </verify>
  <done>
- Backfill engine pages through GitHub API with rate limiting
- Resume logic works via sync_state
- Bot filtering applies to comments
- Thread grouping produces correct chunks
- Embeddings generated for each chunk with fail-open on errors
- Verbose logging per batch
- Single-PR sync function works
  </done>
</task>

<task type="auto">
  <name>Task 2: Create CLI entry point and update barrel exports</name>
  <files>scripts/backfill-review-comments.ts, src/knowledge/index.ts</files>
  <action>
**CLI script (`scripts/backfill-review-comments.ts`):**

Create a CLI entry point runnable as `bun scripts/backfill-review-comments.ts` (and add `"backfill:reviews"` npm script).

The CLI:
1. Parses arguments:
   - `--repo xbmc/xbmc` (default: `xbmc/xbmc`)
   - `--months 18` (default: 18)
   - `--pr 1234` (optional: sync single PR instead of full backfill)
   - `--dry-run` (optional: fetch and log but don't store)
2. Creates dependencies:
   - `createDbClient()` + `runMigrations()`
   - `createReviewCommentStore({ sql, logger })`
   - `createEmbeddingProvider()` with VOYAGE_API_KEY from env
   - GitHub Octokit via `createGitHubApp()` then `getInstallationOctokit()`
3. If `--pr` provided: calls `syncSinglePR()` for that PR
4. Otherwise: calls `backfillReviewComments()` for full backfill
5. Logs the `BackfillResult` on completion
6. Closes DB connection

Wire the GitHub App auth the same way as `scripts/migrate-sqlite-to-postgres.ts` — use env vars directly since this runs outside the server context. Use `createGitHubApp(config, logger)` and `getInstallationOctokit()` for the target repo.

Add to `package.json` scripts:
```json
"backfill:reviews": "bun scripts/backfill-review-comments.ts"
```

**Barrel exports (`src/knowledge/index.ts`):**

Add exports for the new modules:
```typescript
export { createReviewCommentStore } from "./review-comment-store.ts";
export { chunkReviewThread } from "./review-comment-chunker.ts";
export type { ReviewCommentStore, ReviewCommentChunk, ReviewCommentSearchResult, ReviewCommentRecord } from "./review-comment-types.ts";
```
  </action>
  <verify>
- `bun scripts/backfill-review-comments.ts --help` shows usage (or runs without error with --dry-run)
- `grep "backfill:reviews" package.json` confirms npm script is registered
- `src/knowledge/index.ts` exports compile cleanly
- `bun test` all tests pass
  </verify>
  <done>
- CLI script wires all dependencies and invokes backfill engine
- npm script `backfill:reviews` registered in package.json
- Single-PR sync available via `--pr` flag
- Barrel exports updated with all new review comment modules
  </done>
</task>

</tasks>

<verification>
- Backfill engine correctly pages through GitHub API with rate limiting
- Resume logic persists and restores cursor state
- Bot comments are filtered per user decision
- Thread grouping and chunking produce correct outputs
- CLI script can be invoked and connects to GitHub API and PostgreSQL
- All tests pass including existing suite
</verification>

<success_criteria>
- `npm run backfill:reviews` can be executed to populate 18 months of review comments
- Backfill is idempotent and resumable
- Rate limiting stays at ~50% of GitHub authenticated limit
- Every batch is logged with counts and running totals
</success_criteria>

<output>
After completion, create `.planning/phases/89-pr-review-comment-ingestion/89-02-SUMMARY.md`
</output>
