---
phase: 30-state-memory-and-isolation-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/learning/types.ts
  - src/learning/embedding-provider.ts
  - src/learning/memory-store.ts
  - src/learning/isolation.ts
  - src/execution/config.ts
  - src/execution/config.test.ts
autonomous: true

must_haves:
  truths:
    - "Learning memory writes are stored with embeddings and metadata scoped to the originating repository"
    - "Embedding generation fails open -- review publishes without memory if Voyage AI call fails"
    - "Retrieval for a repo cannot read memory from any other repo unless explicit sharing is enabled"
    - "Owner-level sharing opt-in retrieves from shared pool filtered by owner"
    - "Full provenance is logged showing which repos contributed to each retrieval result"
  artifacts:
    - path: "src/learning/types.ts"
      provides: "LearningMemoryRecord, EmbeddingResult, RetrievalResult, MemoryOutcome types"
      contains: "LearningMemoryRecord"
    - path: "src/learning/embedding-provider.ts"
      provides: "Fail-open Voyage AI embedding generation with retry/timeout"
      contains: "generateEmbedding"
    - path: "src/learning/memory-store.ts"
      provides: "vec0 virtual table management, writeMemory, retrieveMemories, purgeStaleEmbeddings"
      contains: "learning_memory_vec"
    - path: "src/learning/isolation.ts"
      provides: "Repo-scoped retrieval, owner-level shared pool queries, provenance logging"
      contains: "retrieveWithIsolation"
    - path: "src/execution/config.ts"
      provides: "Extended knowledge config schema with sharing and embeddings sections"
      contains: "sharing"
  key_links:
    - from: "src/learning/memory-store.ts"
      to: "src/learning/embedding-provider.ts"
      via: "writeMemory calls generateEmbedding for vector"
      pattern: "generateEmbedding"
    - from: "src/learning/isolation.ts"
      to: "src/learning/memory-store.ts"
      via: "retrieveWithIsolation delegates to memory-store retrieval"
      pattern: "retrieveMemories"
    - from: "src/learning/memory-store.ts"
      to: "sqlite-vec vec0 virtual table"
      via: "SQL MATCH queries with repo partition key"
      pattern: "learning_memory_vec"
---

<objective>
Create the learning memory infrastructure: embedding provider, vector-backed memory store, isolation logic, and config extensions.

Purpose: Build the LEARN-06 and REL-03 foundation -- sqlite-vec for vector storage with repo partition keys, Voyage AI for embedding generation (fail-open), and isolation enforcement with owner-level sharing opt-in. These modules are standalone and will be wired into the review handler in Plan 03.
Output: `src/learning/` module with types, embedding provider, memory store, and isolation; extended config schema.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/30-state-memory-and-isolation-foundation/30-CONTEXT.md
@.planning/phases/30-state-memory-and-isolation-foundation/30-RESEARCH.md
@src/execution/config.ts
@src/execution/config.test.ts
@src/knowledge/store.ts
@src/knowledge/types.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies, create learning types, and extend config schema</name>
  <files>package.json, src/learning/types.ts, src/execution/config.ts, src/execution/config.test.ts</files>
  <action>
**Install dependencies:**
```bash
bun add sqlite-vec voyageai
```

**Create src/learning/types.ts:**

```typescript
import type { FindingSeverity, FindingCategory } from "../knowledge/types.ts";

export type MemoryOutcome = 'accepted' | 'suppressed' | 'thumbs_up' | 'thumbs_down';

export type LearningMemoryRecord = {
  id?: number;
  repo: string;
  owner: string;
  findingId: number;
  reviewId: number;
  sourceRepo: string;
  findingText: string;
  severity: FindingSeverity;
  category: FindingCategory;
  filePath: string;
  outcome: MemoryOutcome;
  embeddingModel: string;
  embeddingDim: number;
  stale: boolean;
  createdAt?: string;
};

export type EmbeddingResult = {
  embedding: Float32Array;
  model: string;
  dimensions: number;
} | null;

export type RetrievalResult = {
  memoryId: number;
  distance: number;
  record: LearningMemoryRecord;
  sourceRepo: string;
};

export type RetrievalWithProvenance = {
  results: RetrievalResult[];
  provenance: {
    repoSources: string[];
    sharedPoolUsed: boolean;
    totalCandidates: number;
    query: {
      repo: string;
      topK: number;
      threshold: number;
    };
  };
};

export type EmbeddingConfig = {
  enabled: boolean;
  model: string;
  dimensions: number;
};

export type SharingConfig = {
  enabled: boolean;
};

export type LearningMemoryStore = {
  writeMemory(record: LearningMemoryRecord, embedding: Float32Array): void;
  retrieveMemories(params: {
    queryEmbedding: Float32Array;
    repo: string;
    topK: number;
  }): { memoryId: number; distance: number }[];
  getMemoryRecord(memoryId: number): LearningMemoryRecord | null;
  markStale(embeddingModel: string): number;
  purgeStaleEmbeddings(): number;
  close(): void;
};

export type EmbeddingProvider = {
  generate(text: string, inputType: 'document' | 'query'): Promise<EmbeddingResult>;
  readonly model: string;
  readonly dimensions: number;
};
```

**Extend config schema (src/execution/config.ts):**

Replace the existing `knowledgeSchema`:

```typescript
const embeddingsSchema = z
  .object({
    enabled: z.boolean().default(true),
    model: z.string().default("voyage-code-3"),
    dimensions: z.number().min(256).max(2048).default(1024),
  })
  .default({ enabled: true, model: "voyage-code-3", dimensions: 1024 });

const sharingSchema = z
  .object({
    enabled: z.boolean().default(false),
  })
  .default({ enabled: false });

const knowledgeSchema = z
  .object({
    shareGlobal: z.boolean().default(false),
    sharing: sharingSchema,
    embeddings: embeddingsSchema,
  })
  .default({
    shareGlobal: false,
    sharing: { enabled: false },
    embeddings: { enabled: true, model: "voyage-code-3", dimensions: 1024 },
  });
```

This preserves backward compatibility: the old `shareGlobal` field still works. The new `sharing.enabled` supersedes it (sharing is enabled if either `shareGlobal` or `sharing.enabled` is true).

**Update Pass-2 knowledge fallback** in `loadRepoConfig` to match the new default shape.

**Update config tests (src/execution/config.test.ts):**
Add tests verifying:
1. Default config has `knowledge.embeddings.enabled === true` and `knowledge.embeddings.model === "voyage-code-3"`
2. Default config has `knowledge.sharing.enabled === false`
3. Custom embeddings config parses correctly: `knowledge: { embeddings: { enabled: false, model: "voyage-code-2", dimensions: 512 } }`
4. Custom sharing config: `knowledge: { sharing: { enabled: true } }`
5. Backward compat: `knowledge: { shareGlobal: true }` still parses without error
  </action>
  <verify>Run `bun test src/execution/config.test.ts` -- all config tests pass including new ones. Verify `bun add sqlite-vec voyageai` completed successfully by checking package.json has both dependencies.</verify>
  <done>sqlite-vec and voyageai are installed. Learning types module exists at src/learning/types.ts. Config schema accepts knowledge.embeddings and knowledge.sharing sections with backward-compatible defaults. All config tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Create embedding provider, memory store, and isolation module</name>
  <files>src/learning/embedding-provider.ts, src/learning/memory-store.ts, src/learning/isolation.ts</files>
  <action>
**Create src/learning/embedding-provider.ts:**

Factory function `createEmbeddingProvider(opts: { apiKey: string; model: string; dimensions: number; logger: Logger }): EmbeddingProvider`.

Implementation:
- Import `VoyageAIClient` from `voyageai`.
- `generate(text, inputType)`: Call `client.embed({ input: text, model, inputType }, { timeoutInSeconds: 10, maxRetries: 2 })`. On success, return `{ embedding: new Float32Array(response.data[0].embedding), model, dimensions }`. On ANY error (VoyageAIError or otherwise), log a warning with `(fail-open)` suffix and return `null`. Never throw.
- If `apiKey` is empty/undefined, create a no-op provider that always returns null and logs once at creation.
- Export a `createNoOpEmbeddingProvider(logger: Logger): EmbeddingProvider` that always returns null (for when embeddings are disabled or API key is missing). It should still have `model` and `dimensions` properties set to `"none"` and `0`.

**Create src/learning/memory-store.ts:**

Factory function `createLearningMemoryStore(opts: { db: Database; logger: Logger }): LearningMemoryStore`.

Implementation:
1. At creation, try to load sqlite-vec: `import * as sqliteVec from "sqlite-vec"; sqliteVec.load(db);`. Wrap in try/catch -- if it fails, log error with "sqlite-vec extension failed to load, learning memory disabled (fail-open)" and return a no-op store where all methods are no-ops / return empty arrays.
2. Verify extension loaded: `db.prepare("SELECT vec_version() AS v").get()`. Log the version.
3. Create the metadata table:
```sql
CREATE TABLE IF NOT EXISTS learning_memories (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  repo TEXT NOT NULL,
  owner TEXT NOT NULL,
  finding_id INTEGER,
  review_id INTEGER,
  source_repo TEXT NOT NULL,
  finding_text TEXT NOT NULL,
  severity TEXT NOT NULL,
  category TEXT NOT NULL,
  file_path TEXT NOT NULL,
  outcome TEXT NOT NULL,
  embedding_model TEXT NOT NULL,
  embedding_dim INTEGER NOT NULL,
  stale INTEGER NOT NULL DEFAULT 0,
  created_at TEXT NOT NULL DEFAULT (datetime('now')),
  UNIQUE(repo, finding_id, outcome)
);
CREATE INDEX IF NOT EXISTS idx_memories_repo ON learning_memories(repo);
CREATE INDEX IF NOT EXISTS idx_memories_owner ON learning_memories(owner);
CREATE INDEX IF NOT EXISTS idx_memories_stale ON learning_memories(stale);
```
4. Create the vec0 virtual table:
```sql
CREATE VIRTUAL TABLE IF NOT EXISTS learning_memory_vec USING vec0(
  memory_id INTEGER PRIMARY KEY,
  embedding float[1024],
  repo TEXT partition key,
  severity TEXT,
  category TEXT
);
```
Note: The dimension (1024) is fixed at table creation. If a different dimension is configured, log a warning. For v0.5, we fix at 1024 and document that changing dimensions requires table recreation.

5. `writeMemory(record, embedding)`: Insert into learning_memories (returning id), then insert into learning_memory_vec with the id, embedding via vec_f32(), repo, severity, category. Use a transaction.
6. `retrieveMemories(params)`: KNN query on learning_memory_vec with `WHERE embedding MATCH ? AND k = ? AND repo = ?`, joining back to learning_memories to filter `stale = 0`. Return `{ memoryId, distance }[]`.
7. `getMemoryRecord(memoryId)`: SELECT from learning_memories WHERE id = memoryId.
8. `markStale(embeddingModel)`: UPDATE learning_memories SET stale = 1 WHERE embedding_model != embeddingModel. Return rows affected.
9. `purgeStaleEmbeddings()`: DELETE from learning_memory_vec WHERE memory_id IN (SELECT id FROM learning_memories WHERE stale = 1), then DELETE from learning_memories WHERE stale = 1. Return count.
10. `close()`: No-op (db lifecycle managed by caller).

**Create src/learning/isolation.ts:**

Factory function `createIsolationLayer(opts: { memoryStore: LearningMemoryStore; logger: Logger }): { retrieveWithIsolation: ... }`.

`retrieveWithIsolation(params: { queryEmbedding: Float32Array; repo: string; owner: string; sharingEnabled: boolean; topK: number; distanceThreshold: number; logger: Logger }): RetrievalWithProvenance`:

1. Always query repo-scoped memories first: `memoryStore.retrieveMemories({ queryEmbedding, repo, topK })`.
2. Filter results by distanceThreshold (lower distance = more similar in vec0).
3. If `sharingEnabled`, also query memories where `owner = params.owner` but `repo != params.repo`. This requires a second query to the memory store -- add a `retrieveMemoriesForOwner` method to LearningMemoryStore that queries across repos for same owner. Actually, since vec0 partition key is `repo`, shared pool needs a different approach: query each repo that belongs to the owner. For simplicity in v0.5, do a regular SQL query on learning_memories where `owner = ? AND repo != ? AND stale = 0` to get memory IDs, then look up their records. We don't do KNN on the shared pool in this phase -- that would require either querying multiple partitions or a separate unpartitioned vec0 table. Instead, include shared pool as a future enhancement. For now, shared pool retrieval is metadata-based (same severity+category matches), NOT vector-based.

   Actually, per research recommendations: use partition key per repo. For owner-level sharing, we need to iterate repos or use a separate query. The simplest approach: if sharing enabled, also query `retrieveMemories` with a list of repos belonging to same owner. But we don't know all repos for an owner ahead of time.

   **Pragmatic approach for v0.5:** When sharing is enabled, do a SQL query on `learning_memories` to find distinct repos for the same owner, then query vec0 for each (up to 5 most-active repos by memory count). Merge results, dedupe by memory_id, sort by distance, take top-K total.

4. Build provenance: list unique sourceRepo values from results, note whether shared pool was used.
5. Log provenance at debug level including contributing repos.
6. Return `RetrievalWithProvenance`.

Add `retrieveMemoriesForOwner` to `LearningMemoryStore` interface and implementation: query learning_memories for repos with same owner, then for each repo (up to 5), call retrieveMemories and merge results.
  </action>
  <verify>Run `bun test` -- full test suite passes (no new test file in this task, but existing tests must not break from new imports/modules). Verify `src/learning/embedding-provider.ts`, `src/learning/memory-store.ts`, and `src/learning/isolation.ts` all exist and export their factory functions. Run `bun build src/learning/memory-store.ts --target bun` to verify it compiles without errors.</verify>
  <done>Learning memory infrastructure is complete: embedding provider wraps Voyage AI with fail-open, memory store manages vec0 virtual table with repo partition key, isolation layer enforces repo-scoped retrieval with optional owner-level shared pool and full provenance. All modules follow fail-open patterns. Full test suite passes.</done>
</task>

</tasks>

<verification>
1. `bun test` -- full test suite green
2. `sqlite-vec` and `voyageai` in package.json dependencies
3. `src/learning/types.ts` exports LearningMemoryRecord, EmbeddingProvider, LearningMemoryStore types
4. `src/learning/embedding-provider.ts` exports createEmbeddingProvider and createNoOpEmbeddingProvider
5. `src/learning/memory-store.ts` exports createLearningMemoryStore with vec0 table creation
6. `src/learning/isolation.ts` exports createIsolationLayer with repo-scoped retrieval
7. Config schema accepts `knowledge.embeddings` and `knowledge.sharing` sections
8. Embedding provider returns null (not throw) on any failure
9. Memory store degrades to no-op if sqlite-vec fails to load
</verification>

<success_criteria>
- sqlite-vec and voyageai installed in package.json
- Learning types define all memory, embedding, and retrieval contracts
- Embedding provider wraps Voyage AI with fail-open (null on error, never throws)
- Memory store creates learning_memories table + learning_memory_vec vec0 virtual table
- vec0 uses repo as partition key for automatic isolation
- Isolation layer enforces repo-scoped retrieval with optional owner-level sharing
- Provenance tracks which repos contributed to each retrieval
- Config schema extended with knowledge.sharing and knowledge.embeddings
- All existing tests pass, config tests cover new schema sections
</success_criteria>

<output>
After completion, create `.planning/phases/30-state-memory-and-isolation-foundation/30-02-SUMMARY.md`
</output>
