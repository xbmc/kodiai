---
phase: 86-postgresql-pgvector-on-azure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/provision-postgres.sh
  - src/db/client.ts
  - src/db/migrations/001-initial-schema.sql
  - src/db/migrations/001-initial-schema.down.sql
  - src/db/migrations/002-pgvector-indexes.sql
  - src/db/migrations/002-pgvector-indexes.down.sql
  - src/db/migrations/003-tsvector-columns.sql
  - src/db/migrations/003-tsvector-columns.down.sql
  - src/db/migrate.ts
  - docker-compose.yml
autonomous: true
requirements:
  - DB-01
  - DB-02
  - DB-08
  - DB-09

must_haves:
  truths:
    - "Azure CLI script provisions PostgreSQL Flexible Server with pgvector enabled"
    - "Database schema defines all tables matching current SQLite structure with PostgreSQL idioms"
    - "HNSW indexes exist on vector columns with tuned parameters"
    - "tsvector columns and GIN indexes exist for full-text search"
    - "Migration runner applies versioned SQL files in order and tracks state"
    - "Rollback function reverts migrations to a target version using paired down files"
    - "postgres.js client connects via DATABASE_URL and exports a reusable sql tagged-template instance"
  artifacts:
    - path: "scripts/provision-postgres.sh"
      provides: "Azure PostgreSQL provisioning with pgvector"
    - path: "src/db/client.ts"
      provides: "postgres.js connection singleton"
      exports: ["sql", "connectDb", "closeDb"]
    - path: "src/db/migrate.ts"
      provides: "Migration runner with up and rollback functions"
      exports: ["runMigrations", "runRollback"]
    - path: "src/db/migrations/001-initial-schema.down.sql"
      provides: "Rollback for initial schema â€” drops all tables and pgvector extension"
    - path: "src/db/migrations/002-pgvector-indexes.down.sql"
      provides: "Rollback for HNSW indexes"
    - path: "src/db/migrations/003-tsvector-columns.down.sql"
      provides: "Rollback for tsvector columns, triggers, and GIN indexes"
    - path: "src/db/migrations/001-initial-schema.sql"
      provides: "Core tables: reviews, findings, suppression_log, global_patterns, feedback_reactions, run_state, author_cache, dep_bump_merge_history, review_checkpoints, telemetry_events, rate_limit_events, retrieval_quality_events, resilience_events, learning_memories"
    - path: "src/db/migrations/002-pgvector-indexes.sql"
      provides: "HNSW indexes on embedding columns"
    - path: "src/db/migrations/003-tsvector-columns.sql"
      provides: "tsvector columns and GIN indexes"
    - path: "docker-compose.yml"
      provides: "Local PostgreSQL + pgvector for development and testing"
  key_links:
    - from: "src/db/client.ts"
      to: "DATABASE_URL env var"
      via: "postgres.js connection string"
      pattern: "postgres\\(.*DATABASE_URL"
    - from: "src/db/migrate.ts"
      to: "src/db/migrations/*.sql"
      via: "fs.readdir + sql file execution"
      pattern: "migrations.*\\.sql"
---

<objective>
Create the PostgreSQL foundation: Azure provisioning script, unified schema with all tables from knowledge/telemetry/learning stores, pgvector HNSW indexes, tsvector full-text search columns, a versioned migration runner, and a postgres.js client module.

Purpose: Establish the database layer that all subsequent plans build on. No SQLite code is changed yet -- this plan only creates new PostgreSQL infrastructure.

Output: Provisioning script, docker-compose for local dev, migration SQL files, migration runner, and postgres.js client module.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/knowledge/types.ts
@src/knowledge/store.ts
@src/learning/types.ts
@src/learning/memory-store.ts
@src/telemetry/store.ts
@src/telemetry/types.ts
@src/index.ts
@Dockerfile
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Azure provisioning script, Docker Compose, and postgres.js client</name>
  <files>
    scripts/provision-postgres.sh
    docker-compose.yml
    src/db/client.ts
    package.json
  </files>
  <action>
1. **Add postgres dependency** to package.json: `postgres` (the postgres.js library -- NOT pg, NOT drizzle, NOT kysely). postgres.js is chosen because it provides tagged-template SQL (similar to prepared statements), zero dependencies, and native TypeScript support. It works with Bun natively.

2. **Create `scripts/provision-postgres.sh`** -- an idempotent Azure CLI script that:
   - Creates a resource group (or reuses existing `kodiai-rg`)
   - Provisions an Azure Database for PostgreSQL Flexible Server, Burstable B1ms tier
   - Enables the `vector` extension (pgvector) via `az postgres flexible-server parameter set --name azure.extensions --value VECTOR`
   - Creates a database named `kodiai`
   - Outputs the connection string in `postgresql://` format
   - Uses variables at the top for easy customization (server name, location, admin user/pass, etc.)
   - Make it executable (`chmod +x`)

3. **Create `docker-compose.yml`** for local development and CI testing:
   - Service: `postgres` using `pgvector/pgvector:pg17` image
   - Environment: POSTGRES_DB=kodiai, POSTGRES_USER=kodiai, POSTGRES_PASSWORD=kodiai
   - Port mapping: 5432:5432
   - Named volume for data persistence
   - Healthcheck: `pg_isready -U kodiai -d kodiai`

4. **Create `src/db/client.ts`**:
   - Import `postgres` from `postgres` package
   - Export a `createDbClient(opts: { connectionString?: string; logger: Logger })` factory function
   - Read connection string from `opts.connectionString ?? process.env.DATABASE_URL`
   - Configure: `max: 10`, `idle_timeout: 20`, `connect_timeout: 10`
   - Export `closeDb()` that calls `sql.end()`
   - Export the `sql` instance type for use in store modules
   - If DATABASE_URL is not set, throw a clear error message

Do NOT use `bun:sqlite` or any SQLite import in this file.
  </action>
  <verify>
`bun run docker compose up -d` starts PostgreSQL. `bunx tsc --noEmit` passes. The provision script is syntactically valid (`bash -n scripts/provision-postgres.sh`).
  </verify>
  <done>
postgres.js is in package.json, Docker Compose starts pgvector locally, provision script exists for Azure, and `src/db/client.ts` exports a typed postgres.js client factory.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create unified PostgreSQL schema migrations with pgvector indexes and tsvector columns</name>
  <files>
    src/db/migrations/001-initial-schema.sql
    src/db/migrations/001-initial-schema.down.sql
    src/db/migrations/002-pgvector-indexes.sql
    src/db/migrations/002-pgvector-indexes.down.sql
    src/db/migrations/003-tsvector-columns.sql
    src/db/migrations/003-tsvector-columns.down.sql
    src/db/migrate.ts
  </files>
  <action>
1. **Create `src/db/migrations/001-initial-schema.sql`** -- the unified schema that consolidates all 3 SQLite stores into one PostgreSQL database:

   ```sql
   -- Migration tracking table
   CREATE TABLE IF NOT EXISTS _migrations (
     id SERIAL PRIMARY KEY,
     name TEXT NOT NULL UNIQUE,
     applied_at TIMESTAMPTZ NOT NULL DEFAULT now()
   );

   -- Enable pgvector extension
   CREATE EXTENSION IF NOT EXISTS vector;

   -- Knowledge store tables (from src/knowledge/store.ts)
   CREATE TABLE IF NOT EXISTS reviews ( ... );
   CREATE TABLE IF NOT EXISTS findings ( ... );
   CREATE TABLE IF NOT EXISTS suppression_log ( ... );
   CREATE TABLE IF NOT EXISTS global_patterns ( ... );
   CREATE TABLE IF NOT EXISTS feedback_reactions ( ... );
   CREATE TABLE IF NOT EXISTS run_state ( ... );
   CREATE TABLE IF NOT EXISTS author_cache ( ... );
   CREATE TABLE IF NOT EXISTS dep_bump_merge_history ( ... );
   CREATE TABLE IF NOT EXISTS review_checkpoints ( ... );

   -- Telemetry store tables (from src/telemetry/store.ts)
   CREATE TABLE IF NOT EXISTS telemetry_events ( ... );
   CREATE TABLE IF NOT EXISTS rate_limit_events ( ... );
   CREATE TABLE IF NOT EXISTS retrieval_quality_events ( ... );
   CREATE TABLE IF NOT EXISTS resilience_events ( ... );

   -- Learning memory tables (from src/learning/memory-store.ts)
   CREATE TABLE IF NOT EXISTS learning_memories ( ... );
   ```

   **Schema translation rules** (SQLite -> PostgreSQL):
   - `INTEGER PRIMARY KEY AUTOINCREMENT` -> `SERIAL PRIMARY KEY` (or `BIGSERIAL` for high-volume tables)
   - `TEXT NOT NULL DEFAULT (datetime('now'))` -> `TIMESTAMPTZ NOT NULL DEFAULT now()`
   - `INTEGER NOT NULL DEFAULT 0` for booleans -> `BOOLEAN NOT NULL DEFAULT false`
   - `UNIQUE(...)` constraints stay the same
   - All indexes from the SQLite schema must be recreated with PostgreSQL syntax
   - `ON CONFLICT ... DO UPDATE SET` -> PostgreSQL `ON CONFLICT ... DO UPDATE SET` (same syntax)
   - Remove SQLite-specific functions: `datetime('now', ...)` -> PostgreSQL `now() - INTERVAL '...'`
   - For the learning_memories embedding column: add `embedding vector(1024)` column directly on the learning_memories table (inline, not separate vec0 virtual table). This replaces the sqlite-vec virtual table approach.

2. **Create `src/db/migrations/002-pgvector-indexes.sql`**:
   - Create HNSW index on learning_memories.embedding column:
     ```sql
     CREATE INDEX IF NOT EXISTS idx_learning_memories_embedding_hnsw
       ON learning_memories USING hnsw (embedding vector_cosine_ops)
       WITH (m = 16, ef_construction = 64);
     ```
   - `m = 16` (default, good balance of recall/speed for <100K vectors)
   - `ef_construction = 64` (default, sufficient for bot workload)
   - Use `vector_cosine_ops` distance operator (matching VoyageAI cosine similarity)

3. **Create `src/db/migrations/003-tsvector-columns.sql`**:
   - Add `search_tsv tsvector` column to `learning_memories` table (generated from `finding_text`)
   - Add GIN index on the tsvector column
   - Create a trigger to auto-update the tsvector column on INSERT/UPDATE
   - Add `search_tsv tsvector` column to `findings` table (generated from `title`)
   - Add GIN index on findings.search_tsv

4. **Create paired down migration files** (per locked decision: "versioned up/down migrations"):

   **`src/db/migrations/001-initial-schema.down.sql`**:
   - Drop all tables in reverse dependency order: `learning_memories`, `resilience_events`, `retrieval_quality_events`, `rate_limit_events`, `telemetry_events`, `review_checkpoints`, `dep_bump_merge_history`, `author_cache`, `run_state`, `feedback_reactions`, `global_patterns`, `suppression_log`, `findings`, `reviews`
   - Drop `_migrations` table last
   - Drop pgvector extension: `DROP EXTENSION IF EXISTS vector`
   - Use `DROP TABLE IF EXISTS` for idempotency

   **`src/db/migrations/002-pgvector-indexes.down.sql`**:
   - `DROP INDEX IF EXISTS idx_learning_memories_embedding_hnsw`

   **`src/db/migrations/003-tsvector-columns.down.sql`**:
   - Drop triggers that auto-update tsvector columns
   - Drop GIN indexes on `search_tsv` columns
   - `ALTER TABLE learning_memories DROP COLUMN IF EXISTS search_tsv`
   - `ALTER TABLE findings DROP COLUMN IF EXISTS search_tsv`

5. **Create `src/db/migrate.ts`**:
   - Export `async function runMigrations(sql: Sql): Promise<void>`
   - Read all `.sql` files from `src/db/migrations/` directory (excluding `.down.sql` files), sorted by filename
   - For each file: check if already applied (exists in `_migrations` table)
   - If not applied: execute the SQL within a transaction, then insert into `_migrations`
   - Log each migration applied
   - Handle the bootstrap case where `_migrations` table doesn't exist yet (the first migration creates it)
   - Use `import.meta.dir` to resolve the migrations directory path relative to the module

   - Export `async function runRollback(sql: Sql, targetVersion: number): Promise<void>`
   - Query `_migrations` table for all applied migrations, sorted descending by id
   - For each applied migration whose version number (numeric prefix, e.g. `003`) is greater than `targetVersion`:
     a. Derive the down file name: replace `.sql` with `.down.sql` in the migration name
     b. Read and execute the down SQL file within a transaction
     c. Delete the migration row from `_migrations`
     d. Log each rollback applied
   - If `targetVersion` is 0, roll back everything (all migrations)
   - Throw a clear error if a `.down.sql` file is missing for a migration that needs rollback
  </action>
  <verify>
Start Docker Compose postgres, then run: `bun run src/db/migrate.ts` (add a small CLI entry point at bottom that connects and runs migrations with `up` and `down <version>` subcommands). Verify all tables exist with `\dt` in psql. Verify pgvector extension loaded with `SELECT extversion FROM pg_extension WHERE extname = 'vector'`. Verify HNSW index exists. Verify tsvector columns and GIN indexes exist. Then test rollback: `bun run src/db/migrate.ts down 2` should remove tsvector columns/indexes but leave tables and HNSW index. `bun run src/db/migrate.ts up` should re-apply migration 003. `bun run src/db/migrate.ts down 0` should drop everything.
  </verify>
  <done>
Three up migration files and three paired down migration files define the complete PostgreSQL schema with rollback support. Migration runner applies them idempotently. `runRollback(sql, targetVersion)` reverts to any prior state. Running migrations against Docker Compose PostgreSQL creates all tables, pgvector extension, HNSW indexes, tsvector columns, and GIN indexes. Rolling back to version 0 drops everything cleanly.
  </done>
</task>

</tasks>

<verification>
1. `docker compose up -d` starts PostgreSQL with pgvector extension
2. `bun run src/db/migrate.ts` applies all migrations without error
3. `psql` shows all expected tables, indexes, and extensions
4. `bunx tsc --noEmit` passes with no type errors
5. No `bun:sqlite` imports in any new files
</verification>

<success_criteria>
- PostgreSQL running locally via Docker Compose with pgvector
- Azure provisioning script ready for manual execution
- All tables from knowledge, telemetry, and learning stores defined in PostgreSQL
- HNSW index on embedding column with m=16, ef_construction=64
- tsvector columns with GIN indexes on learning_memories and findings
- Migration runner applies SQL files idempotently with tracking
- Paired down migrations exist for every up migration
- runRollback() reverts to any target version using down files
- postgres.js client module exports typed connection factory
</success_criteria>

<output>
After completion, create `.planning/phases/86-postgresql-pgvector-on-azure/86-01-SUMMARY.md`
</output>
