---
phase: 90-mediawiki-content-ingestion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/migrations/006-wiki-pages.sql
  - src/db/migrations/006-wiki-pages.down.sql
  - src/knowledge/wiki-types.ts
  - src/knowledge/wiki-store.ts
  - src/knowledge/wiki-store.test.ts
  - src/knowledge/wiki-chunker.ts
  - src/knowledge/wiki-chunker.test.ts
autonomous: true
requirements: [KI-07, KI-08, KI-09]

must_haves:
  truths:
    - "knowledge.wiki_pages table exists in PostgreSQL with pgvector embedding column and all required metadata columns"
    - "Wiki pages can be stored with full metadata: page title, section heading, last modified date, URL, and namespace"
    - "Section-based chunking splits pages at heading boundaries (## / ###) with 1024-token sliding window and 256-token overlap for large sections"
    - "HNSW index on wiki_pages.embedding enables cosine similarity search"
    - "Wiki markup (tables, templates, infoboxes) is stripped to plain text; code blocks preserved as-is"
    - "Page title + section heading prepended as prefix to chunk text before embedding"
  artifacts:
    - path: "src/db/migrations/006-wiki-pages.sql"
      provides: "knowledge.wiki_pages table with pgvector embedding column, metadata columns, indexes"
      contains: "wiki_pages"
    - path: "src/knowledge/wiki-store.ts"
      provides: "CRUD operations for wiki page chunks with vector storage and retrieval"
      contains: "createWikiPageStore"
    - path: "src/knowledge/wiki-types.ts"
      provides: "Type definitions for wiki page records, chunks, and store interface"
      contains: "WikiPageChunk"
    - path: "src/knowledge/wiki-chunker.ts"
      provides: "Section-based chunking with heading split and 1024-token windows with 256-token overlap"
      contains: "chunkWikiPage"
  key_links:
    - from: "src/knowledge/wiki-store.ts"
      to: "src/db/client.ts"
      via: "Uses shared Sql connection pool from createDbClient"
      pattern: "Sql|sql`"
    - from: "src/knowledge/wiki-store.ts"
      to: "src/knowledge/wiki-types.ts"
      via: "Store implements WikiPageStore interface"
      pattern: "WikiPageStore|WikiPageChunk"
    - from: "src/db/migrations/006-wiki-pages.sql"
      to: "src/db/migrations/002-pgvector-indexes.sql"
      via: "HNSW index on embedding column uses same vector_cosine_ops pattern"
      pattern: "hnsw|vector_cosine_ops"
---

<objective>
Create the PostgreSQL schema, store module, and chunking logic for MediaWiki content ingestion.

Purpose: Establish the data layer that backfill (Plan 02) and retrieval integration (Plan 03) depend on. Schema must support full metadata, vector search, and section-level granularity.
Output: Migration file, store module with write/read/search operations, type definitions, and section-based chunker with HTML-to-markdown conversion.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@src/db/migrations/001-initial-schema.sql
@src/db/migrations/002-pgvector-indexes.sql
@src/db/migrations/005-review-comments.sql
@src/db/migrate.ts
@src/db/client.ts
@src/knowledge/memory-store.ts
@src/knowledge/review-comment-store.ts
@src/knowledge/review-comment-types.ts
@src/knowledge/review-comment-chunker.ts
@src/knowledge/types.ts
@src/knowledge/embeddings.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create wiki_pages schema migration and type definitions</name>
  <files>src/db/migrations/006-wiki-pages.sql, src/db/migrations/006-wiki-pages.down.sql, src/knowledge/wiki-types.ts</files>
  <action>
Create migration `006-wiki-pages.sql` with the `wiki_pages` table:

```sql
CREATE TABLE IF NOT EXISTS knowledge.wiki_pages (
  id BIGSERIAL PRIMARY KEY,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),

  -- Page identity
  page_id INTEGER NOT NULL,           -- MediaWiki page ID
  page_title TEXT NOT NULL,           -- Full page title (e.g. "Settings/Audio")
  namespace TEXT NOT NULL DEFAULT '', -- MediaWiki namespace (Main, Add-ons, Development, etc.)
  page_url TEXT NOT NULL,             -- Full URL to kodi.wiki page

  -- Section metadata
  section_heading TEXT,               -- Section heading within the page (NULL for intro/lead section)
  section_anchor TEXT,                -- URL anchor for deep linking (e.g. "#Audio_output")
  section_level INTEGER,              -- Heading depth (2 for ##, 3 for ###, etc.)

  -- Content
  chunk_index INTEGER NOT NULL DEFAULT 0,  -- 0 for single-chunk section, 0..N for split sections
  chunk_text TEXT NOT NULL,                -- Chunked text for this embedding (title + section prefix prepended)
  raw_text TEXT NOT NULL,                  -- Original text before prefix prepending
  token_count INTEGER NOT NULL,

  -- Embedding
  embedding vector(1024),            -- voyage-code-3 embedding (same as learning_memories and review_comments)
  embedding_model TEXT,
  stale BOOLEAN NOT NULL DEFAULT false,

  -- Freshness
  last_modified TIMESTAMPTZ,         -- MediaWiki page last modified timestamp
  revision_id INTEGER,               -- MediaWiki revision ID for change detection

  -- Lifecycle
  deleted BOOLEAN NOT NULL DEFAULT false,  -- Soft delete for removed pages

  UNIQUE(page_id, section_anchor, chunk_index)
);
```

Add indexes:
- `idx_wiki_pages_page_id` on `(page_id)` -- page-level lookups and deletion
- `idx_wiki_pages_namespace` on `(namespace)` -- namespace filtering
- `idx_wiki_pages_title` on `(page_title)` -- title lookups
- `idx_wiki_pages_embedding_hnsw` using HNSW on `(embedding vector_cosine_ops)` with `(m = 16, ef_construction = 64)` -- same tuning as learning_memories and review_comments
- `idx_wiki_pages_stale` on `(stale)` WHERE `stale = false` -- active-only partial index
- tsvector GIN index on `chunk_text` for BM25-style full-text search (follow pattern from 003-tsvector-columns.sql)

Add a `wiki_sync_state` table for tracking sync progress:
```sql
CREATE TABLE IF NOT EXISTS knowledge.wiki_sync_state (
  id SERIAL PRIMARY KEY,
  source TEXT NOT NULL UNIQUE,          -- e.g. "kodi.wiki" (future-proof for multiple wikis)
  last_synced_at TIMESTAMPTZ,           -- Timestamp of most recent sync completion
  last_continue_token TEXT,             -- MediaWiki API continue token for resume
  total_pages_synced INTEGER NOT NULL DEFAULT 0,
  backfill_complete BOOLEAN NOT NULL DEFAULT false,
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
```

Create the `.down.sql` rollback file dropping both tables and indexes.

Create `src/knowledge/wiki-types.ts` with:
- `WikiPageInput` -- raw MediaWiki API response fields before chunking:
  - `pageId: number`, `pageTitle: string`, `namespace: string`, `pageUrl: string`
  - `htmlContent: string` (raw HTML from MediaWiki API)
  - `lastModified?: Date`, `revisionId?: number`
- `WikiPageChunk` -- the unit stored/embedded:
  - `pageId: number`, `pageTitle: string`, `namespace: string`, `pageUrl: string`
  - `sectionHeading?: string`, `sectionAnchor?: string`, `sectionLevel?: number`
  - `chunkIndex: number`, `chunkText: string`, `rawText: string`, `tokenCount: number`
  - `lastModified?: Date`, `revisionId?: number`
  - `embedding?: Float32Array | null`
- `WikiPageRecord` -- full DB row type with all columns
- `WikiPageSearchResult` -- search result with distance score: `{ record: WikiPageRecord; distance: number }`
- `WikiSyncState` -- sync tracking record type:
  - `id?: number`, `source: string`, `lastSyncedAt: Date | null`, `lastContinueToken: string | null`
  - `totalPagesSynced: number`, `backfillComplete: boolean`, `updatedAt?: string`
- `WikiPageStore` interface with methods:
  - `writeChunks(chunks: WikiPageChunk[]): Promise<void>` -- bulk upsert
  - `deletePageChunks(pageId: number): Promise<void>` -- delete all chunks for a page (for re-chunking on edit)
  - `replacePageChunks(pageId: number, chunks: WikiPageChunk[]): Promise<void>` -- delete + insert in transaction
  - `softDeletePage(pageId: number): Promise<void>` -- soft delete all chunks for page
  - `searchByEmbedding(params: { queryEmbedding: Float32Array; topK: number; namespace?: string }): Promise<WikiPageSearchResult[]>`
  - `getPageChunks(pageId: number): Promise<WikiPageRecord[]>`
  - `getSyncState(source: string): Promise<WikiSyncState | null>`
  - `updateSyncState(state: WikiSyncState): Promise<void>`
  - `countBySource(): Promise<number>` -- stats
  - `getPageRevision(pageId: number): Promise<number | null>` -- for change detection
  </action>
  <verify>
- `bun run src/db/migrate.ts up` applies the migration without error
- TypeScript compiles: `bun build --no-bundle src/knowledge/wiki-types.ts`
- Type definitions export all required interfaces
  </verify>
  <done>
- wiki_pages table exists with all metadata columns, embedding column, and indexes
- wiki_sync_state table exists for tracking sync progress
- Down migration cleanly drops both tables
- All type definitions and interfaces exported
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement wiki page store and section-based chunker</name>
  <files>src/knowledge/wiki-store.ts, src/knowledge/wiki-store.test.ts, src/knowledge/wiki-chunker.ts, src/knowledge/wiki-chunker.test.ts</files>
  <action>
**Chunker (`wiki-chunker.ts`):**

Implement `chunkWikiPage()` with section-based splitting and sliding window per user decision: 1024-token windows, 256-token overlap.

Algorithm:
1. Accept a `WikiPageInput` record with raw HTML content
2. **HTML-to-markdown conversion:**
   - Strip all HTML tags except for headings (`<h2>`, `<h3>`, `<h4>`) and code blocks (`<pre>`, `<code>`)
   - Convert `<h2>` through `<h4>` to markdown headings (## through ####)
   - Convert `<pre><code>` blocks to markdown fenced code blocks (preserve content as-is per user decision)
   - Convert `<table>` elements to text rows: extract `<td>` / `<th>` content, join cells with " | ", rows with newlines
   - Remove template/infobox markup (MediaWiki `{{...}}` patterns if present in HTML)
   - Strip remaining HTML tags, decode HTML entities (`&amp;` -> `&`, etc.)
   - Collapse multiple blank lines to single blank line
3. **Skip filtering (per user decision):**
   - Skip if page is a redirect (check for `#REDIRECT` in raw text)
   - Skip if page appears to be a stub: < 500 characters after HTML stripping
   - Skip disambiguation pages (check for common disambiguation markers like "may refer to" or category "Disambiguation")
4. **Section splitting:**
   - Split the cleaned markdown at section headings (lines matching `^#{2,4}\s+`)
   - The text before the first heading is the "lead section" (section_heading = null, section_anchor = null)
   - Each section: extract heading text, compute anchor (lowercase, spaces to underscores), and section level
5. **Sliding window within sections:**
   - Count tokens using whitespace-based approximation (`text.split(/\s+/).length`) -- same as review-comment-chunker
   - If section tokens <= 1024: produce a single chunk with `chunk_index = 0`
   - If section tokens > 1024: slide a window of 1024 tokens with 256-token overlap, producing chunks with `chunk_index = 0, 1, 2, ...`
6. **Prefix prepending (per user decision):**
   - Prepend page title + section heading to chunk text: `"{page_title} > {section_heading}: {chunk_text}"`
   - For lead section (no heading): `"{page_title}: {chunk_text}"`
   - Store the pre-prefix text in `rawText` for display, and the prefixed version in `chunkText` for embedding
7. Each chunk carries full page metadata (pageId, pageTitle, namespace, pageUrl, lastModified, revisionId)

Export: `chunkWikiPage(page: WikiPageInput): WikiPageChunk[]`
Also export: `stripHtmlToMarkdown(html: string): string` for independent testing of conversion
Also export: `countTokens(text: string): number` helper (reuse from review-comment-chunker or duplicate -- it's a one-liner)

**Store (`wiki-store.ts`):**

Implement `createWikiPageStore({ sql, logger })` returning `WikiPageStore`. Follow the same factory pattern as `createReviewCommentStore`.

Key implementation details:
- `writeChunks`: Batch INSERT with `ON CONFLICT (page_id, section_anchor, chunk_index) DO NOTHING` for idempotent backfill. Handle NULL section_anchor for lead sections by using empty string as sentinel in the unique constraint.
- `deletePageChunks`: DELETE FROM wiki_pages WHERE page_id = $1
- `replacePageChunks`: Transaction wrapping deletePageChunks + writeChunks (for re-ingestion on page edit)
- `softDeletePage`: UPDATE `deleted = true` WHERE `page_id = $1`
- `searchByEmbedding`: Use `embedding <=> $queryVector::vector` for cosine distance, filter by `stale = false`, `deleted = false`, optional `namespace` filter, `embedding IS NOT NULL`, ORDER BY distance, LIMIT topK. Return `WikiPageSearchResult` with distance, chunk text, and all metadata.
- `getSyncState`/`updateSyncState`: CRUD on `wiki_sync_state` (UPSERT on source)
- `countBySource`: Simple COUNT of non-deleted rows
- `getPageRevision`: `SELECT revision_id FROM wiki_pages WHERE page_id = $1 AND deleted = false LIMIT 1`
- `getPageChunks`: `SELECT * FROM wiki_pages WHERE page_id = $1 AND deleted = false ORDER BY chunk_index`

Use the same `float32ArrayToVectorString()` helper pattern from `review-comment-store.ts` for embedding serialization.

**Tests:**

For the chunker:
- Simple page with no headings produces lead-section chunks
- Page with multiple sections splits at headings
- Large section (>1024 tokens) produces multiple overlapping chunks with correct overlap
- Redirect pages return zero chunks
- Stub pages (<500 chars after stripping) return zero chunks
- HTML tables converted to text rows
- Code blocks preserved as-is in markdown format
- Page title + section heading prepended to chunk text
- Section anchors correctly generated from headings
- HTML entities decoded correctly
- Empty page returns zero chunks

For the store (needs PostgreSQL -- use `beforeAll`/`afterAll` with real DB connection):
- writeChunks stores and retrieves by page ID
- writeChunks is idempotent (re-run does not duplicate)
- deletePageChunks removes all chunks for a page
- replacePageChunks atomically replaces chunks
- softDeletePage marks all page chunks as deleted
- searchByEmbedding returns results sorted by distance
- searchByEmbedding filters by namespace when provided
- getSyncState/updateSyncState round-trips
- countBySource returns correct count
- getPageRevision returns latest revision for page
  </action>
  <verify>
- `bun test src/knowledge/wiki-chunker.test.ts` passes
- `bun test src/knowledge/wiki-store.test.ts` passes (requires DATABASE_URL)
- All existing tests still pass: `bun test`
  </verify>
  <done>
- Chunker correctly handles section splitting and sliding window with 1024/256 windowing
- HTML-to-markdown conversion strips markup while preserving code blocks
- Page title + section prefix prepended to chunks for embedding context
- Store implements full WikiPageStore interface with vector search
- All tests pass including existing suite
  </done>
</task>

</tasks>

<verification>
- Migration 006 applies and rolls back cleanly
- `wiki_pages` table has embedding column, HNSW index, tsvector GIN index, and all metadata columns
- `wiki_sync_state` table supports sync progress tracking
- Chunker produces correct section-based splitting with 1024/256 overlap
- HTML-to-markdown conversion handles tables, code blocks, entities
- Store write/read/search operations work with real PostgreSQL
- All existing tests continue to pass
</verification>

<success_criteria>
- New migration applies without breaking existing schema
- Store can write, read, search, and soft-delete wiki page chunks
- Chunker handles section splitting and sliding-window within sections
- Redirect/stub/disambiguation pages are filtered out
- Type definitions cover all required interfaces
</success_criteria>

<output>
After completion, create `.planning/phases/90-mediawiki-content-ingestion/90-01-SUMMARY.md`
</output>
