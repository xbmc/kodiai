---
phase: 97-multi-llm-routing-cost-tracking
plan: 03
type: execute
wave: 2
depends_on:
  - 97-01
  - 97-02
files_modified:
  - src/llm/generate.ts
  - src/llm/fallback.ts
  - src/llm/index.ts
  - src/execution/executor.ts
  - src/handlers/review.ts
  - src/handlers/mention.ts
  - src/slack/assistant.ts
autonomous: true
requirements:
  - LLM-01
  - LLM-04

must_haves:
  truths:
    - "Non-agentic tasks complete via AI SDK generateText() using the configured model"
    - "Agentic tasks (PR review, mentions, Slack) still use Agent SDK query() by default"
    - "Changing models: in .kodiai.yml causes a different model to be used on next invocation"
    - "When configured provider is unavailable (429, 5xx, timeout), task falls back to default model"
    - "Fallback triggers a visible annotation in the output"
    - "If all models fail, optional signals degrade gracefully; core tasks fail hard"
    - "Every AI SDK and Agent SDK invocation is cost-tracked via CostTracker"
  artifacts:
    - path: "src/llm/generate.ts"
      provides: "Unified generate wrapper with fallback and cost tracking"
      exports: ["generateWithFallback", "GenerateResult"]
    - path: "src/llm/fallback.ts"
      provides: "Fallback trigger detection logic"
      exports: ["isFallbackTrigger"]
    - path: "src/execution/executor.ts"
      provides: "Modified executor with task router integration and Agent SDK cost tracking"
      contains: "costTracker"
  key_links:
    - from: "src/llm/generate.ts"
      to: "src/llm/task-router.ts"
      via: "TaskRouter.resolve() for model selection"
      pattern: "router\\.resolve"
    - from: "src/llm/generate.ts"
      to: "src/llm/cost-tracker.ts"
      via: "CostTracker.trackAiSdkCall() after each invocation"
      pattern: "trackAiSdkCall"
    - from: "src/execution/executor.ts"
      to: "src/llm/cost-tracker.ts"
      via: "CostTracker.trackAgentSdkCall() after Agent SDK query()"
      pattern: "trackAgentSdkCall"
    - from: "src/llm/generate.ts"
      to: "src/llm/fallback.ts"
      via: "isFallbackTrigger() to detect 429/5xx/timeout"
      pattern: "isFallbackTrigger"
---

<objective>
Build the generateText wrapper with fallback logic and wire the task routing + cost tracking into the existing executor and handler call sites.

Purpose: Complete the routing layer by connecting the foundation (Plan 01/02) to actual LLM call sites. After this plan, non-agentic tasks route through AI SDK, agentic tasks continue on Agent SDK, all calls are cost-tracked, and fallback handles provider failures.
Output: Working generate wrapper, modified executor with cost tracking, handler integration points.
</objective>

<execution_context>
@/home/keith/.claude/get-shit-done/workflows/execute-plan.md
@/home/keith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/97-multi-llm-routing-cost-tracking/97-CONTEXT.md
@.planning/phases/97-multi-llm-routing-cost-tracking/97-RESEARCH.md
@.planning/phases/97-multi-llm-routing-cost-tracking/97-01-SUMMARY.md
@.planning/phases/97-multi-llm-routing-cost-tracking/97-02-SUMMARY.md
@src/execution/executor.ts
@src/llm/index.ts

<interfaces>
<!-- Key contracts from Plan 01 and Plan 02. -->

From src/llm/task-router.ts (Plan 01):
```typescript
export interface ResolvedModel {
  modelId: string;
  provider: string;
  sdk: "agent" | "ai";
  fallbackModelId: string;
  fallbackProvider: string;
}

export function createTaskRouter(config: {
  models: Record<string, string>;
  defaultModel?: string;
  defaultFallbackModel?: string;
}): { resolve(taskType: string): ResolvedModel };
```

From src/llm/providers.ts (Plan 01):
```typescript
export function createProviderModel(modelId: string): LanguageModel;
export function extractProvider(modelId: string): string;
```

From src/llm/cost-tracker.ts (Plan 02):
```typescript
export type CostTracker = {
  trackAiSdkCall(params: { ... }): Promise<void>;
  trackAgentSdkCall(params: { ... }): Promise<void>;
};
export function createCostTracker(deps: {
  telemetryStore: TelemetryStore;
  logger: Logger;
}): CostTracker;
```

From src/execution/executor.ts (existing):
```typescript
export function createExecutor(deps: {
  githubApp: GitHubApp;
  logger: Logger;
}): { execute(context: ExecutionContext): Promise<ExecutionResult> };
```

From src/execution/types.ts (existing):
```typescript
export type ExecutionContext = {
  workspace: { dir: string; cleanup: () => Promise<void> };
  // ... many fields
  modelOverride?: string;
  // ...
};
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create fallback detection and generateWithFallback wrapper</name>
  <files>
    src/llm/fallback.ts
    src/llm/generate.ts
    src/llm/index.ts
  </files>
  <action>
**Step 1: Create `src/llm/fallback.ts`.**

Per CONTEXT.md locked decisions on fallback behavior:
```typescript
/**
 * Determines whether an error should trigger fallback to alternative model.
 * Per user decision: 429 rate limits trigger immediate fallback (no retry).
 * 5xx errors and timeouts also trigger fallback.
 */
export function isFallbackTrigger(err: unknown): boolean {
  if (!(err instanceof Error)) return false;

  // Check HTTP status in error properties (AI SDK wraps provider errors)
  const status = (err as any).status ?? (err as any).statusCode
    ?? (err as any).data?.status;

  if (status === 429) return true;  // Rate limit -> immediate fallback
  if (typeof status === "number" && status >= 500 && status < 600) return true;

  // Timeout detection
  if (err.message.includes("timeout") || err.name === "AbortError") return true;

  return false;
}

/**
 * Extracts a human-readable fallback reason from the error.
 */
export function getFallbackReason(err: unknown): string {
  if (!(err instanceof Error)) return "unknown error";
  const status = (err as any).status ?? (err as any).statusCode;
  if (status === 429) return "rate limited (429)";
  if (typeof status === "number" && status >= 500) return `server error (${status})`;
  if (err.message.includes("timeout") || err.name === "AbortError") return "timeout";
  return err.message;
}
```

**Step 2: Create `src/llm/generate.ts`.**

Build the unified generateText wrapper per research Pattern 2:

```typescript
import { generateText } from "ai";
import type { Logger } from "pino";
```

Export types:
```typescript
export interface GenerateResult {
  text: string;
  usage: { inputTokens: number; outputTokens: number };
  model: string;
  provider: string;
  usedFallback: boolean;
  fallbackReason?: string;
  /** Visible annotation to include in output when fallback was used */
  fallbackAnnotation?: string;
  durationMs: number;
}
```

Export main function:
```typescript
export async function generateWithFallback(opts: {
  taskType: string;
  resolved: ResolvedModel;
  prompt: string;
  system?: string;
  tools?: Record<string, any>;
  stopWhen?: any;
  costTracker?: CostTracker;
  repo?: string;
  deliveryId?: string;
  logger: Logger;
}): Promise<GenerateResult>
```

Implementation:
1. Create provider model via `createProviderModel(opts.resolved.modelId)`
2. Record `startTime = Date.now()`
3. Try `generateText({ model, prompt, system, tools, stopWhen })` -- NEVER use `streamText()` (Bun bug)
4. On success: build `GenerateResult` from response, call `costTracker.trackAiSdkCall()` fire-and-forget, return result
5. On error: check `isFallbackTrigger(err)`:
   - If yes AND `opts.resolved.fallbackModelId` exists:
     - Log warning: "Primary model {modelId} failed ({reason}), falling back to {fallbackModelId}"
     - Create fallback provider model
     - Retry with fallback model
     - Set `usedFallback: true`, `fallbackReason` from `getFallbackReason(err)`
     - Set `fallbackAnnotation: "> **Note:** Used fallback model \`${fallbackModelId}\` (configured provider unavailable: ${reason})"`
     - Track cost with `usedFallback: true`
   - If no fallback available or fallback also fails:
     - Check if task type is agentic (`isAgenticTaskType`): if yes, rethrow (core tasks fail hard)
     - If non-agentic: throw with a descriptive error (caller handles graceful degradation)
6. Always track cost in finally-like logic (even partial on error)

IMPORTANT: Do NOT import or use `streamText`. Only use `generateText`.

**Step 3: Update `src/llm/index.ts` barrel.**

Add exports for:
- From fallback: isFallbackTrigger, getFallbackReason
- From generate: generateWithFallback, GenerateResult
  </action>
  <verify>
    <automated>cd /home/keith/src/kodiai && bun run tsc --noEmit 2>&1 | head -30</automated>
  </verify>
  <done>generateWithFallback wraps AI SDK generateText() with fallback on 429/5xx/timeout. Fallback annotates output with visible note. Non-agentic failures degrade gracefully. Every invocation tracked via CostTracker. No streamText usage anywhere.</done>
</task>

<task type="auto">
  <name>Task 2: Wire task router and cost tracking into executor and handlers</name>
  <files>
    src/execution/executor.ts
    src/handlers/review.ts
    src/handlers/mention.ts
    src/slack/assistant.ts
  </files>
  <action>
**Step 1: Modify `src/execution/executor.ts` to integrate cost tracking.**

Add CostTracker as an optional dependency to `createExecutor`:
```typescript
export function createExecutor(deps: {
  githubApp: GitHubApp;
  logger: Logger;
  costTracker?: CostTracker;
}) {
```

After the Agent SDK `query()` completes and `resultMessage` is available (around line 192, after extracting modelEntries/totalInput/totalOutput/etc.), add cost tracking:

```typescript
// Track Agent SDK cost (fire-and-forget, fail-open)
if (deps.costTracker) {
  const taskType = context.taskType ?? "review.full"; // default for backward compat
  deps.costTracker.trackAgentSdkCall({
    repo: `${context.owner}/${context.repo}`,
    taskType,
    model: primaryModel,
    inputTokens: totalInput,
    outputTokens: totalOutput,
    cacheReadTokens: totalCacheRead,
    cacheWriteTokens: totalCacheCreation,
    durationMs: resultMessage.duration_ms ?? durationMs,
    costUsd: resultMessage.total_cost_usd,
    deliveryId: context.deliveryId,
  });
}
```

Also add `taskType` to ExecutionContext type in `src/execution/types.ts`:
```typescript
/** Task type for LLM routing and cost tracking (e.g., "review.full", "mention.response"). */
taskType?: string;
```

Modify the model selection line (line 36) to use TaskRouter when available. The executor receives a `taskRouter` in deps (optional for backward compat):
```typescript
export function createExecutor(deps: {
  githubApp: GitHubApp;
  logger: Logger;
  costTracker?: CostTracker;
  taskRouter?: { resolve(taskType: string): ResolvedModel };
}) {
```

In the execute method, after loading config, resolve the model:
```typescript
const taskType = context.taskType ?? "review.full";
let model: string;
if (deps.taskRouter) {
  const resolved = deps.taskRouter.resolve(taskType);
  // For agentic tasks routed to Agent SDK, use resolved model
  // For agentic tasks routed to AI SDK (user override), still use Agent SDK but with the model
  model = context.modelOverride ?? resolved.modelId;
  logger.info({ taskType, resolvedModel: resolved.modelId, sdk: resolved.sdk, provider: resolved.provider }, "Task router resolved model");
} else {
  model = context.modelOverride ?? config.model;
}
```

**Step 2: Thread taskType through handler call sites.**

In `src/handlers/review.ts`:
- Find where `executor.execute()` is called
- Set `context.taskType = "review.full"` before calling execute

In `src/handlers/mention.ts`:
- Set `context.taskType = "mention.response"` before calling execute

In `src/slack/assistant.ts`:
- Set `context.taskType = "slack.response"` before calling execute

These are minimal changes: just adding a `taskType` field to the context object passed to `executor.execute()`. Search for `executor.execute(` or `.execute({` to find the exact call sites.

**IMPORTANT constraints:**
- Do NOT change the Agent SDK `query()` call structure. The model is already passed to `query()` via the `model` option. The task router just influences which model string is used.
- Do NOT replace Agent SDK with AI SDK for agentic tasks by default. The router's `sdk` field is for future use -- in v1, agentic tasks always go through Agent SDK `query()`.
- Keep the executor backward compatible: all new deps are optional.
  </action>
  <verify>
    <automated>cd /home/keith/src/kodiai && bun run tsc --noEmit 2>&1 | head -30</automated>
  </verify>
  <done>Executor uses TaskRouter for model resolution and CostTracker for Agent SDK cost logging. All three handler types (review, mention, Slack) set taskType on ExecutionContext. All new deps are optional for backward compatibility. TypeScript compiles clean.</done>
</task>

</tasks>

<verification>
- `bun run tsc --noEmit` passes with zero errors
- `src/llm/generate.ts` uses `generateText` (not `streamText`)
- `src/execution/executor.ts` calls `costTracker.trackAgentSdkCall()` after Agent SDK execution
- `src/execution/types.ts` includes `taskType?: string` on ExecutionContext
- Handler files set taskType before executor.execute()
- Fallback annotation is visible in generate result when fallback triggers
</verification>

<success_criteria>
- generateWithFallback calls generateText with primary model, then fallback on 429/5xx/timeout
- Fallback includes visible annotation: "Used fallback model (configured provider unavailable)"
- Executor integrates TaskRouter.resolve() for model selection
- Executor integrates CostTracker.trackAgentSdkCall() for Agent SDK cost logging
- Review handler sets taskType="review.full", mention handler sets taskType="mention.response", Slack sets taskType="slack.response"
- All changes are backward compatible (optional deps)
</success_criteria>

<output>
After completion, create `.planning/phases/97-multi-llm-routing-cost-tracking/97-03-SUMMARY.md`
</output>
